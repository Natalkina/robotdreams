[2024-02-25T11:36:32.739+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: processing_sales.transform_data_to_silver scheduled__2022-09-01T01:00:00+00:00 [queued]>
[2024-02-25T11:36:32.749+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: processing_sales.transform_data_to_silver scheduled__2022-09-01T01:00:00+00:00 [queued]>
[2024-02-25T11:36:32.749+0000] {taskinstance.py:2171} INFO - Starting attempt 2 of 4
[2024-02-25T11:36:32.764+0000] {taskinstance.py:2192} INFO - Executing <Task(BigQueryInsertJobOperator): transform_data_to_silver> on 2022-09-01 01:00:00+00:00
[2024-02-25T11:36:32.775+0000] {standard_task_runner.py:60} INFO - Started process 546 to run task
[2024-02-25T11:36:32.780+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'processing_sales', 'transform_data_to_silver', 'scheduled__2022-09-01T01:00:00+00:00', '--job-id', '114', '--raw', '--subdir', 'DAGS_FOLDER/process_sales.py', '--cfg-path', '/tmp/tmpce_pg4er']
[2024-02-25T11:36:32.782+0000] {standard_task_runner.py:88} INFO - Job 114: Subtask transform_data_to_silver
[2024-02-25T11:36:32.836+0000] {task_command.py:423} INFO - Running <TaskInstance: processing_sales.transform_data_to_silver scheduled__2022-09-01T01:00:00+00:00 [running]> on host 9e4f31030bba
[2024-02-25T11:36:32.912+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='processing_sales' AIRFLOW_CTX_TASK_ID='transform_data_to_silver' AIRFLOW_CTX_EXECUTION_DATE='2022-09-01T01:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2022-09-01T01:00:00+00:00'
[2024-02-25T11:36:32.926+0000] {connection.py:234} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2024-02-25T11:36:32.931+0000] {base.py:83} INFO - Using connection ID 'google_cloud_default' for task execution.
[2024-02-25T11:36:32.933+0000] {bigquery.py:2806} INFO - Executing: {'query': {'query': "\n                    INSERT INTO `de-07-natalia-sokil.silver.sales` (client_id, purchase_date, product_name, price)\n                    SELECT\n                        CustomerId AS client_id,\n                        PARSE_DATE('%Y-%m-%d', PurchaseDate) AS purchase_date,\n                        Product AS product_name,\n                        CAST(Price AS FLOAT64) AS price\n                    FROM `de-07-nataliia-sokil.bronze.sales`\n                ", 'useLegacySql': False}}'
[2024-02-25T11:36:32.980+0000] {bigquery.py:1596} INFO - Inserting job ***_processing_sales_transform_data_to_silver_2022_09_01T01_00_00_00_00_331448914f82f6cae17de696e1e73728
[2024-02-25T11:36:33.654+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2864, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1595, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1584, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 952, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.NotFound: 404 Not found: Table de-07-natalia-sokil:silver.sales was not found in location EU

Location: EU
Job ID: airflow_processing_sales_transform_data_to_silver_2022_09_01T01_00_00_00_00_331448914f82f6cae17de696e1e73728

[2024-02-25T11:36:33.663+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=processing_sales, task_id=transform_data_to_silver, execution_date=20220901T010000, start_date=20240225T113632, end_date=20240225T113633
[2024-02-25T11:36:33.676+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 114 for task transform_data_to_silver (404 Not found: Table de-07-natalia-sokil:silver.sales was not found in location EU

Location: EU
Job ID: airflow_processing_sales_transform_data_to_silver_2022_09_01T01_00_00_00_00_331448914f82f6cae17de696e1e73728
; 546)
[2024-02-25T11:36:33.716+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-25T11:36:33.733+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-02-25T11:59:37.593+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: processing_sales.transform_data_to_silver scheduled__2022-09-01T01:00:00+00:00 [queued]>
[2024-02-25T11:59:37.606+0000] {taskinstance.py:1957} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: processing_sales.transform_data_to_silver scheduled__2022-09-01T01:00:00+00:00 [queued]>
[2024-02-25T11:59:37.607+0000] {taskinstance.py:2171} INFO - Starting attempt 2 of 5
[2024-02-25T11:59:37.624+0000] {taskinstance.py:2192} INFO - Executing <Task(BigQueryInsertJobOperator): transform_data_to_silver> on 2022-09-01 01:00:00+00:00
[2024-02-25T11:59:37.631+0000] {standard_task_runner.py:60} INFO - Started process 903 to run task
[2024-02-25T11:59:37.636+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'processing_sales', 'transform_data_to_silver', 'scheduled__2022-09-01T01:00:00+00:00', '--job-id', '120', '--raw', '--subdir', 'DAGS_FOLDER/process_sales.py', '--cfg-path', '/tmp/tmpcbpyimeb']
[2024-02-25T11:59:37.638+0000] {standard_task_runner.py:88} INFO - Job 120: Subtask transform_data_to_silver
[2024-02-25T11:59:37.694+0000] {task_command.py:423} INFO - Running <TaskInstance: processing_sales.transform_data_to_silver scheduled__2022-09-01T01:00:00+00:00 [running]> on host 9e4f31030bba
[2024-02-25T11:59:37.788+0000] {taskinstance.py:2481} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='processing_sales' AIRFLOW_CTX_TASK_ID='transform_data_to_silver' AIRFLOW_CTX_EXECUTION_DATE='2022-09-01T01:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2022-09-01T01:00:00+00:00'
[2024-02-25T11:59:37.801+0000] {connection.py:234} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2024-02-25T11:59:37.807+0000] {base.py:83} INFO - Using connection ID 'google_cloud_default' for task execution.
[2024-02-25T11:59:37.809+0000] {bigquery.py:2806} INFO - Executing: {'query': {'query': "\n                    INSERT INTO `de-07-natalia-sokil.silver.sales` (client_id, purchase_date, product_name, price)\n                    SELECT\n                        CustomerId AS client_id,\n                        PARSE_DATE('%Y-%m-%d', PurchaseDate) AS purchase_date,\n                        Product AS product_name,\n                        CAST(Price AS FLOAT64) AS price\n                    FROM `de-07-natalia-sokil.bronze.sales`\n                ", 'useLegacySql': False}}'
[2024-02-25T11:59:37.859+0000] {bigquery.py:1596} INFO - Inserting job ***_processing_sales_transform_data_to_silver_2022_09_01T01_00_00_00_00_a6deb21e31d805b9acc333b4d4b9870d
[2024-02-25T11:59:39.677+0000] {taskinstance.py:2699} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2864, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1595, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1584, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 952, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.BadRequest: 400 Bad double value: 730$

Location: EU
Job ID: airflow_processing_sales_transform_data_to_silver_2022_09_01T01_00_00_00_00_a6deb21e31d805b9acc333b4d4b9870d

[2024-02-25T11:59:39.689+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=processing_sales, task_id=transform_data_to_silver, execution_date=20220901T010000, start_date=20240225T115937, end_date=20240225T115939
[2024-02-25T11:59:39.737+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 120 for task transform_data_to_silver (400 Bad double value: 730$

Location: EU
Job ID: airflow_processing_sales_transform_data_to_silver_2022_09_01T01_00_00_00_00_a6deb21e31d805b9acc333b4d4b9870d
; 903)
[2024-02-25T11:59:39.778+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-02-25T11:59:39.796+0000] {taskinstance.py:3281} INFO - 0 downstream tasks scheduled from follow-on schedule check
