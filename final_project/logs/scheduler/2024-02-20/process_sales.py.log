[2024-02-20T18:12:25.932+0000] {processor.py:161} INFO - Started process (PID=34) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:12:25.934+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:12:25.937+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:12:25.937+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:12:27.231+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:12:27.227+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:12:27.233+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:12:27.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 1.326 seconds
[2024-02-20T18:12:57.562+0000] {processor.py:161} INFO - Started process (PID=97) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:12:57.564+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:12:57.566+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:12:57.566+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:12:58.035+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:12:58.029+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:12:58.037+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:12:58.061+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.503 seconds
[2024-02-20T18:13:29.736+0000] {processor.py:161} INFO - Started process (PID=156) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:13:29.737+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:13:29.738+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:13:29.738+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:13:30.264+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:13:30.259+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:13:30.266+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:13:30.286+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.555 seconds
[2024-02-20T18:14:00.943+0000] {processor.py:161} INFO - Started process (PID=219) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:14:00.944+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:14:00.946+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:14:00.945+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:14:01.436+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:14:01.430+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:14:01.438+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:14:01.494+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.555 seconds
[2024-02-20T18:14:31.742+0000] {processor.py:161} INFO - Started process (PID=277) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:14:31.743+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:14:31.745+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:14:31.745+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:14:32.317+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:14:32.311+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:14:32.319+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:14:32.351+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.616 seconds
[2024-02-20T18:15:02.493+0000] {processor.py:161} INFO - Started process (PID=339) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:15:02.495+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:15:02.497+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:15:02.496+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:15:03.031+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:15:03.026+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:15:03.032+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:15:03.051+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.563 seconds
[2024-02-20T18:15:33.169+0000] {processor.py:161} INFO - Started process (PID=395) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:15:33.170+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:15:33.171+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:15:33.171+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:15:33.616+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:15:33.611+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:15:33.619+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:15:33.641+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.476 seconds
[2024-02-20T18:16:03.857+0000] {processor.py:161} INFO - Started process (PID=457) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:16:03.859+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:16:03.860+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:16:03.860+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:16:04.343+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:16:04.336+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:16:04.344+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:16:04.367+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.514 seconds
[2024-02-20T18:16:34.672+0000] {processor.py:161} INFO - Started process (PID=517) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:16:34.673+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:16:34.675+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:16:34.674+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:16:35.097+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:16:35.092+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:16:35.098+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:16:35.120+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.453 seconds
[2024-02-20T18:17:05.210+0000] {processor.py:161} INFO - Started process (PID=576) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:17:05.211+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:17:05.212+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:17:05.212+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:17:05.666+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:17:05.659+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:17:05.667+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:17:05.724+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.518 seconds
[2024-02-20T18:17:35.836+0000] {processor.py:161} INFO - Started process (PID=635) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:17:35.838+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:17:35.839+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:17:35.838+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:17:36.242+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:17:36.236+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:17:36.244+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:17:36.300+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.468 seconds
[2024-02-20T18:18:06.363+0000] {processor.py:161} INFO - Started process (PID=694) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:18:06.364+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:18:06.365+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:18:06.365+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:18:06.872+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:18:06.864+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:18:06.874+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:18:06.896+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.537 seconds
[2024-02-20T18:18:37.571+0000] {processor.py:161} INFO - Started process (PID=754) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:18:37.572+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:18:37.574+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:18:37.574+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:18:38.020+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:18:38.014+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:18:38.022+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:18:38.045+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.480 seconds
[2024-02-20T18:19:08.185+0000] {processor.py:161} INFO - Started process (PID=816) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:19:08.187+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:19:08.188+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:19:08.188+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:19:08.655+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:19:08.648+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:19:08.657+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:19:08.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.509 seconds
[2024-02-20T18:19:39.562+0000] {processor.py:161} INFO - Started process (PID=875) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:19:39.564+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:19:39.565+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:19:39.565+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:19:40.059+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:19:40.054+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 7, in <module>
    from airflow.operators.bigquery_operator import BigQueryOperator
ModuleNotFoundError: No module named 'airflow.operators.bigquery_operator'
[2024-02-20T18:19:40.061+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:19:40.085+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.529 seconds
[2024-02-20T18:19:44.018+0000] {processor.py:161} INFO - Started process (PID=918) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:19:44.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:19:44.023+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:19:44.022+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:19:44.047+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:19:44.044+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 8, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:19:44.048+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:19:44.073+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.060 seconds
[2024-02-20T18:20:14.307+0000] {processor.py:161} INFO - Started process (PID=966) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:20:14.308+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:20:14.311+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:20:14.310+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:20:14.330+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:20:14.326+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 8, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:20:14.332+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:20:14.356+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.053 seconds
[2024-02-20T18:20:15.784+0000] {processor.py:161} INFO - Started process (PID=976) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:20:15.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:20:15.789+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:20:15.789+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:20:15.816+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:20:15.812+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:20:15.817+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:20:15.862+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.083 seconds
[2024-02-20T18:20:45.968+0000] {processor.py:161} INFO - Started process (PID=1024) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:20:45.969+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:20:45.973+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:20:45.973+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:20:45.997+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:20:45.994+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:20:45.999+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:20:46.062+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.099 seconds
[2024-02-20T18:20:56.446+0000] {processor.py:161} INFO - Started process (PID=1035) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:20:56.448+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:20:56.453+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:20:56.452+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:20:56.485+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:20:56.481+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:20:56.486+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:20:56.546+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.105 seconds
[2024-02-20T18:21:26.889+0000] {processor.py:161} INFO - Started process (PID=1092) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:21:26.890+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:21:26.893+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:21:26.893+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:21:26.906+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:21:26.904+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:21:26.907+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:21:26.964+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.079 seconds
[2024-02-20T18:21:57.559+0000] {processor.py:161} INFO - Started process (PID=1148) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:21:57.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:21:57.564+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:21:57.563+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:21:57.583+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:21:57.580+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:21:57.585+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:21:57.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.093 seconds
[2024-02-20T18:22:28.156+0000] {processor.py:161} INFO - Started process (PID=1204) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:22:28.158+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:22:28.161+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:22:28.160+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:22:28.187+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:22:28.181+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:22:28.189+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:22:28.253+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.100 seconds
[2024-02-20T18:22:58.757+0000] {processor.py:161} INFO - Started process (PID=1261) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:22:58.759+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:22:58.763+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:22:58.762+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:22:58.787+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:22:58.782+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:22:58.788+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:22:58.846+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.094 seconds
[2024-02-20T18:23:28.975+0000] {processor.py:161} INFO - Started process (PID=1318) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:23:28.976+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:23:28.980+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:23:28.979+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:23:29.000+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:23:28.996+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:23:29.002+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:23:29.038+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.068 seconds
[2024-02-20T18:23:59.767+0000] {processor.py:161} INFO - Started process (PID=1375) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:23:59.768+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:23:59.771+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:23:59.771+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:23:59.789+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:23:59.785+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:23:59.790+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:23:59.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.085 seconds
[2024-02-20T18:24:30.116+0000] {processor.py:161} INFO - Started process (PID=1432) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:24:30.118+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:24:30.122+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:24:30.121+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:24:30.141+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:24:30.139+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:24:30.143+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:24:30.198+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.086 seconds
[2024-02-20T18:25:00.688+0000] {processor.py:161} INFO - Started process (PID=1489) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:25:00.690+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:25:00.694+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:25:00.693+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:25:00.712+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:25:00.707+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:25:00.714+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:25:00.738+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.055 seconds
[2024-02-20T18:25:31.611+0000] {processor.py:161} INFO - Started process (PID=1545) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:25:31.612+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:25:31.616+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:25:31.616+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:25:31.634+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:25:31.631+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:25:31.636+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:25:31.658+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.052 seconds
[2024-02-20T18:26:01.902+0000] {processor.py:161} INFO - Started process (PID=1602) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:26:01.903+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:26:01.907+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:26:01.907+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:26:01.929+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:26:01.924+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:26:01.930+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:26:01.988+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.091 seconds
[2024-02-20T18:26:32.704+0000] {processor.py:161} INFO - Started process (PID=1659) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:26:32.706+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:26:32.711+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:26:32.711+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:26:32.733+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:26:32.728+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:26:32.735+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:26:32.775+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.076 seconds
[2024-02-20T18:27:02.850+0000] {processor.py:161} INFO - Started process (PID=1716) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:27:02.852+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:27:02.857+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:27:02.857+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:27:02.891+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:27:02.884+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:27:02.893+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:27:02.918+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.073 seconds
[2024-02-20T18:27:33.445+0000] {processor.py:161} INFO - Started process (PID=1773) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:27:33.447+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:27:33.450+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:27:33.450+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:27:33.468+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:27:33.465+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:27:33.469+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:27:33.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.082 seconds
[2024-02-20T18:28:04.383+0000] {processor.py:161} INFO - Started process (PID=1830) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:28:04.384+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:28:04.388+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:28:04.388+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:28:04.404+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:28:04.401+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:28:04.405+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:28:04.427+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.048 seconds
[2024-02-20T18:28:34.651+0000] {processor.py:161} INFO - Started process (PID=1887) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:28:34.653+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:28:34.658+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:28:34.657+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:28:34.679+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:28:34.675+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:28:34.681+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:28:34.708+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.065 seconds
[2024-02-20T18:29:04.879+0000] {processor.py:161} INFO - Started process (PID=1944) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:29:04.880+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:29:04.884+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:29:04.883+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:29:04.898+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:29:04.895+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:29:04.900+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:29:04.922+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.048 seconds
[2024-02-20T18:29:35.781+0000] {processor.py:161} INFO - Started process (PID=2001) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:29:35.782+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:29:35.787+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:29:35.786+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:29:35.806+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:29:35.803+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:29:35.808+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:29:35.828+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.052 seconds
[2024-02-20T18:30:06.710+0000] {processor.py:161} INFO - Started process (PID=2058) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:30:06.711+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:30:06.714+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:30:06.714+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:30:06.732+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:30:06.728+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:30:06.733+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:30:06.757+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.050 seconds
[2024-02-20T18:30:37.195+0000] {processor.py:161} INFO - Started process (PID=2115) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:30:37.196+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:30:37.200+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:30:37.199+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:30:37.218+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:30:37.214+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:30:37.219+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:30:37.277+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.086 seconds
[2024-02-20T18:31:08.045+0000] {processor.py:161} INFO - Started process (PID=2172) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:31:08.046+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:31:08.050+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:31:08.050+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:31:08.068+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:31:08.064+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 9, in <module>
    from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator
ImportError: cannot import name 'BigQueryOperator' from 'airflow.providers.google.cloud.operators.bigquery' (/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py)
[2024-02-20T18:31:08.069+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:31:08.091+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.049 seconds
[2024-02-20T18:31:23.385+0000] {processor.py:161} INFO - Started process (PID=2194) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:31:23.387+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:31:23.390+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:31:23.390+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:31:23.423+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:31:23.417+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'google_cloud_default'}
[2024-02-20T18:31:23.423+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:31:23.447+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.066 seconds
[2024-02-20T18:31:53.539+0000] {processor.py:161} INFO - Started process (PID=2250) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:31:53.541+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:31:53.544+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:31:53.544+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:31:53.567+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:31:53.563+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'google_cloud_default'}
[2024-02-20T18:31:53.568+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:31:53.594+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.060 seconds
[2024-02-20T18:32:23.729+0000] {processor.py:161} INFO - Started process (PID=2306) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:32:23.731+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:32:23.735+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:32:23.734+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:32:23.760+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:32:23.755+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'google_cloud_default'}
[2024-02-20T18:32:23.761+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:32:23.787+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.064 seconds
[2024-02-20T18:32:53.866+0000] {processor.py:161} INFO - Started process (PID=2360) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:32:53.867+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:32:53.870+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:32:53.870+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:32:53.890+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:32:53.886+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'google_cloud_default'}
[2024-02-20T18:32:53.891+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:32:53.914+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.052 seconds
[2024-02-20T18:33:05.580+0000] {processor.py:161} INFO - Started process (PID=2400) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:33:05.582+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:33:05.591+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:33:05.589+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:33:05.660+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:33:05.651+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:33:05.661+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:33:05.694+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.121 seconds
[2024-02-20T18:33:36.021+0000] {processor.py:161} INFO - Started process (PID=2457) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:33:36.022+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:33:36.026+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:33:36.025+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:33:36.060+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:33:36.055+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:33:36.061+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:33:36.085+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.068 seconds
[2024-02-20T18:34:06.327+0000] {processor.py:161} INFO - Started process (PID=2514) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:34:06.328+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:34:06.332+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:34:06.332+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:34:06.364+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:34:06.357+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:34:06.364+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:34:06.428+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.106 seconds
[2024-02-20T18:34:36.570+0000] {processor.py:161} INFO - Started process (PID=2571) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:34:36.571+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:34:36.574+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:34:36.574+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:34:36.606+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:34:36.600+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:34:36.607+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:34:36.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.064 seconds
[2024-02-20T18:35:07.141+0000] {processor.py:161} INFO - Started process (PID=2628) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:35:07.142+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:35:07.145+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:35:07.144+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:35:07.165+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:35:07.161+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:35:07.166+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:35:07.187+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.051 seconds
[2024-02-20T18:35:37.755+0000] {processor.py:161} INFO - Started process (PID=2685) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:35:37.756+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:35:37.759+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:35:37.759+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:35:37.782+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:35:37.778+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:35:37.782+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:35:37.837+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.086 seconds
[2024-02-20T18:36:08.195+0000] {processor.py:161} INFO - Started process (PID=2742) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:36:08.196+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:36:08.199+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:36:08.199+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:36:08.223+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:36:08.219+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:36:08.224+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:36:08.283+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.092 seconds
[2024-02-20T18:36:38.990+0000] {processor.py:161} INFO - Started process (PID=2799) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:36:38.991+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:36:38.995+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:36:38.995+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:36:39.020+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:36:39.016+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:36:39.021+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:36:39.044+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.058 seconds
[2024-02-20T18:37:09.239+0000] {processor.py:161} INFO - Started process (PID=2855) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:37:09.240+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:37:09.243+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:37:09.243+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:37:09.268+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:37:09.263+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:37:09.268+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:37:09.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.056 seconds
[2024-02-20T18:37:39.580+0000] {processor.py:161} INFO - Started process (PID=2912) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:37:39.581+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:37:39.586+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:37:39.585+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:37:39.617+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:37:39.612+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:37:39.618+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:37:39.676+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.101 seconds
[2024-02-20T18:38:10.016+0000] {processor.py:161} INFO - Started process (PID=2969) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:38:10.018+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:38:10.022+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:38:10.021+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:38:10.062+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:38:10.053+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:38:10.062+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:38:10.124+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.113 seconds
[2024-02-20T18:38:41.064+0000] {processor.py:161} INFO - Started process (PID=3026) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:38:41.065+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:38:41.069+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:38:41.068+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:38:41.092+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:38:41.088+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:38:41.093+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:38:41.154+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.095 seconds
[2024-02-20T18:39:11.336+0000] {processor.py:161} INFO - Started process (PID=3083) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:39:11.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:39:11.342+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:39:11.341+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:39:11.369+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:39:11.364+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:39:11.370+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:39:11.400+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.070 seconds
[2024-02-20T18:39:41.536+0000] {processor.py:161} INFO - Started process (PID=3140) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:39:41.537+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:39:41.540+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:39:41.540+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:39:41.567+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:39:41.562+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:39:41.568+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:39:41.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.099 seconds
[2024-02-20T18:40:12.489+0000] {processor.py:161} INFO - Started process (PID=3197) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:40:12.491+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:40:12.496+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:40:12.496+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:40:12.522+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:40:12.517+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:40:12.523+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:40:12.549+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.066 seconds
[2024-02-20T18:40:43.316+0000] {processor.py:161} INFO - Started process (PID=3260) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:40:43.317+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:40:43.321+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:40:43.320+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:40:43.344+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:40:43.340+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:40:43.345+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:40:43.402+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.091 seconds
[2024-02-20T18:41:13.796+0000] {processor.py:161} INFO - Started process (PID=3317) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:41:13.797+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:41:13.800+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:41:13.800+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:41:13.821+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:41:13.817+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:41:13.822+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:41:13.885+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.093 seconds
[2024-02-20T18:41:44.365+0000] {processor.py:161} INFO - Started process (PID=3374) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:41:44.366+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:41:44.369+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:41:44.369+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:41:44.389+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:41:44.385+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:41:44.390+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:41:44.410+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.049 seconds
[2024-02-20T18:42:14.620+0000] {processor.py:161} INFO - Started process (PID=3431) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:42:14.622+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:42:14.625+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:42:14.624+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:42:14.648+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:42:14.644+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:42:14.649+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:42:14.669+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.052 seconds
[2024-02-20T18:42:45.247+0000] {processor.py:161} INFO - Started process (PID=3487) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:42:45.248+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:42:45.251+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:42:45.251+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:42:45.277+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:42:45.272+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:42:45.278+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:42:45.301+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.058 seconds
[2024-02-20T18:43:15.442+0000] {processor.py:161} INFO - Started process (PID=3544) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:43:15.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:43:15.446+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:43:15.446+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:43:15.471+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:43:15.467+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:43:15.472+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:43:15.526+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.089 seconds
[2024-02-20T18:43:45.864+0000] {processor.py:161} INFO - Started process (PID=3601) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:43:45.866+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:43:45.869+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:43:45.869+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:43:45.896+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:43:45.891+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:43:45.896+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:43:45.939+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.079 seconds
[2024-02-20T18:44:16.158+0000] {processor.py:161} INFO - Started process (PID=3658) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:44:16.160+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:44:16.163+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:44:16.163+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:44:16.192+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:44:16.186+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:44:16.193+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:44:16.221+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.067 seconds
[2024-02-20T18:44:46.401+0000] {processor.py:161} INFO - Started process (PID=3715) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:44:46.403+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:44:46.408+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:44:46.407+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:44:46.432+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:44:46.428+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:44:46.433+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:44:46.499+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.105 seconds
[2024-02-20T18:45:16.907+0000] {processor.py:161} INFO - Started process (PID=3772) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:45:16.908+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:45:16.911+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:45:16.911+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:45:16.931+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:45:16.927+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:45:16.932+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:45:16.954+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.051 seconds
[2024-02-20T18:45:47.006+0000] {processor.py:161} INFO - Started process (PID=3822) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:45:47.008+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:45:47.012+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:45:47.012+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:45:47.038+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:45:47.033+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:45:47.039+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:45:47.065+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.065 seconds
[2024-02-20T18:46:17.353+0000] {processor.py:161} INFO - Started process (PID=3879) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:46:17.354+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:46:17.358+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:46:17.357+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:46:17.381+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:46:17.375+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:46:17.382+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:46:17.430+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.082 seconds
[2024-02-20T18:46:47.819+0000] {processor.py:161} INFO - Started process (PID=3936) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:46:47.821+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:46:47.825+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:46:47.825+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:46:47.849+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:46:47.844+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:46:47.850+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:46:47.909+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.096 seconds
[2024-02-20T18:47:18.253+0000] {processor.py:161} INFO - Started process (PID=3993) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:47:18.254+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:47:18.258+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:47:18.258+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:47:18.283+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:47:18.277+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:47:18.284+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:47:18.341+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.094 seconds
[2024-02-20T18:47:48.488+0000] {processor.py:161} INFO - Started process (PID=4050) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:47:48.489+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:47:48.494+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:47:48.493+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:47:48.517+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:47:48.513+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:47:48.518+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:47:48.573+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.090 seconds
[2024-02-20T18:48:18.759+0000] {processor.py:161} INFO - Started process (PID=4107) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:48:18.760+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:48:18.765+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:48:18.764+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:48:18.789+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:48:18.783+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:48:18.790+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:48:18.813+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.059 seconds
[2024-02-20T18:48:49.223+0000] {processor.py:161} INFO - Started process (PID=4164) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:48:49.225+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:48:49.229+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:48:49.229+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:48:49.255+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:48:49.250+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:48:49.256+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:48:49.279+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.061 seconds
[2024-02-20T18:49:19.423+0000] {processor.py:161} INFO - Started process (PID=4221) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:49:19.425+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:49:19.430+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:49:19.430+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:49:19.454+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:49:19.450+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:49:19.455+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:49:19.516+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.099 seconds
[2024-02-20T18:49:49.802+0000] {processor.py:161} INFO - Started process (PID=4278) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:49:49.803+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:49:49.807+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:49:49.807+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:49:49.830+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:49:49.825+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:49:49.830+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:49:49.886+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.090 seconds
[2024-02-20T18:50:20.274+0000] {processor.py:161} INFO - Started process (PID=4335) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:50:20.275+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:50:20.279+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:50:20.278+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:50:20.307+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:50:20.300+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:50:20.308+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:50:20.358+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.089 seconds
[2024-02-20T18:50:50.595+0000] {processor.py:161} INFO - Started process (PID=4392) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:50:50.597+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:50:50.603+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:50:50.603+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:50:50.632+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:50:50.626+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:50:50.633+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:50:50.660+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.071 seconds
[2024-02-20T18:51:21.298+0000] {processor.py:161} INFO - Started process (PID=4449) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:51:21.300+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:51:21.308+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:51:21.307+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:51:21.341+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:51:21.335+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:51:21.342+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:51:21.372+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.082 seconds
[2024-02-20T18:51:44.335+0000] {processor.py:161} INFO - Started process (PID=4477) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:51:44.336+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:51:44.341+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:51:44.340+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:51:44.393+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:51:44.385+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:51:44.394+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:51:44.426+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.097 seconds
[2024-02-20T18:52:14.735+0000] {processor.py:161} INFO - Started process (PID=4540) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:52:14.737+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:52:14.741+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:52:14.740+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:52:14.771+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:52:14.765+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:52:14.772+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:52:14.830+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.101 seconds
[2024-02-20T18:52:44.990+0000] {processor.py:161} INFO - Started process (PID=4597) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:52:44.991+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:52:44.996+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:52:44.995+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:52:45.026+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:52:45.020+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:52:45.027+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:52:45.056+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.072 seconds
[2024-02-20T18:53:15.428+0000] {processor.py:161} INFO - Started process (PID=4654) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:53:15.430+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:53:15.433+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:53:15.433+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:53:15.458+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:53:15.454+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:53:15.459+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:53:15.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.062 seconds
[2024-02-20T18:53:45.781+0000] {processor.py:161} INFO - Started process (PID=4711) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:53:45.782+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:53:45.786+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:53:45.786+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:53:45.816+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:53:45.810+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:53:45.817+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:53:45.879+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.104 seconds
[2024-02-20T18:54:16.059+0000] {processor.py:161} INFO - Started process (PID=4768) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:54:16.061+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:54:16.065+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:54:16.064+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:54:16.090+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:54:16.084+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:54:16.091+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:54:16.152+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.099 seconds
[2024-02-20T18:54:46.403+0000] {processor.py:161} INFO - Started process (PID=4825) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:54:46.404+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:54:46.409+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:54:46.408+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:54:46.433+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:54:46.429+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:54:46.434+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:54:46.497+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.100 seconds
[2024-02-20T18:55:16.672+0000] {processor.py:161} INFO - Started process (PID=4882) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:55:16.673+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:55:16.677+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:55:16.676+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:55:16.712+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:55:16.706+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:55:16.712+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:55:16.780+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.114 seconds
[2024-02-20T18:55:46.849+0000] {processor.py:161} INFO - Started process (PID=4939) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:55:46.850+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:55:46.855+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:55:46.854+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:55:46.881+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:55:46.875+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:55:46.882+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:55:46.946+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.104 seconds
[2024-02-20T18:56:17.019+0000] {processor.py:161} INFO - Started process (PID=4996) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:56:17.020+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:56:17.024+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:56:17.023+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:56:17.056+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:56:17.049+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:56:17.057+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:56:17.081+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.067 seconds
[2024-02-20T18:56:47.490+0000] {processor.py:161} INFO - Started process (PID=5052) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:56:47.491+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:56:47.494+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:56:47.494+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:56:47.521+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:56:47.515+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:56:47.522+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:56:47.580+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.096 seconds
[2024-02-20T18:57:17.920+0000] {processor.py:161} INFO - Started process (PID=5109) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:57:17.921+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:57:17.926+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:57:17.925+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:57:17.957+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:57:17.951+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:57:17.958+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:57:18.022+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.109 seconds
[2024-02-20T18:57:49.038+0000] {processor.py:161} INFO - Started process (PID=5166) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:57:49.039+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:57:49.044+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:57:49.043+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:57:49.072+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:57:49.065+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:57:49.073+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:57:49.130+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.097 seconds
[2024-02-20T18:58:19.370+0000] {processor.py:161} INFO - Started process (PID=5223) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:58:19.372+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:58:19.375+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:58:19.375+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:58:19.402+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:58:19.397+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:58:19.402+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:58:19.462+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.097 seconds
[2024-02-20T18:58:49.581+0000] {processor.py:161} INFO - Started process (PID=5280) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:58:49.582+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:58:49.585+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:58:49.585+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:58:49.612+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:58:49.607+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:58:49.613+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:58:49.671+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.097 seconds
[2024-02-20T18:59:19.805+0000] {processor.py:161} INFO - Started process (PID=5337) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:59:19.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:59:19.809+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:59:19.808+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:59:19.828+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:59:19.824+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:59:19.829+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:59:19.890+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.090 seconds
[2024-02-20T18:59:50.064+0000] {processor.py:161} INFO - Started process (PID=5395) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T18:59:50.065+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T18:59:50.068+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:59:50.068+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T18:59:50.093+0000] {logging_mixin.py:188} INFO - [2024-02-20T18:59:50.088+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T18:59:50.093+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T18:59:50.116+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.058 seconds
[2024-02-20T19:00:20.880+0000] {processor.py:161} INFO - Started process (PID=5452) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:00:20.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:00:20.885+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:00:20.884+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:00:20.907+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:00:20.903+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T19:00:20.908+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:00:20.931+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.056 seconds
[2024-02-20T19:00:51.317+0000] {processor.py:161} INFO - Started process (PID=5509) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:00:51.319+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:00:51.322+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:00:51.322+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:00:51.347+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:00:51.340+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T19:00:51.347+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:00:51.406+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.093 seconds
[2024-02-20T19:01:22.081+0000] {processor.py:161} INFO - Started process (PID=5566) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:01:22.082+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:01:22.086+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:01:22.086+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:01:22.116+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:01:22.111+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T19:01:22.117+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:01:22.140+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.066 seconds
[2024-02-20T19:01:52.732+0000] {processor.py:161} INFO - Started process (PID=5623) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:01:52.734+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:01:52.738+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:01:52.738+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:01:52.762+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:01:52.757+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T19:01:52.763+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:01:52.787+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.059 seconds
[2024-02-20T19:02:23.513+0000] {processor.py:161} INFO - Started process (PID=5680) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:02:23.515+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:02:23.520+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:02:23.519+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:02:23.549+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:02:23.543+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T19:02:23.550+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:02:23.611+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.104 seconds
[2024-02-20T19:02:53.774+0000] {processor.py:161} INFO - Started process (PID=5737) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:02:53.775+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:02:53.780+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:02:53.779+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:02:53.803+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:02:53.799+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T19:02:53.803+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:02:53.827+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.058 seconds
[2024-02-20T19:03:24.191+0000] {processor.py:161} INFO - Started process (PID=5794) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:03:24.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:03:24.197+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:03:24.196+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:03:24.221+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:03:24.216+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T19:03:24.221+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:03:24.244+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.058 seconds
[2024-02-20T19:03:54.558+0000] {processor.py:161} INFO - Started process (PID=5851) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:03:54.560+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:03:54.564+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:03:54.563+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:03:54.588+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:03:54.583+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T19:03:54.589+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:03:54.648+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.096 seconds
[2024-02-20T19:04:24.767+0000] {processor.py:161} INFO - Started process (PID=5908) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:04:24.768+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:04:24.772+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:04:24.772+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:04:24.795+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:04:24.790+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T19:04:24.796+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:04:24.856+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.093 seconds
[2024-02-20T19:04:55.010+0000] {processor.py:161} INFO - Started process (PID=5965) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:04:55.011+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:04:55.015+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:04:55.014+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:04:55.045+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:04:55.039+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T19:04:55.045+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:04:55.072+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.067 seconds
[2024-02-20T19:05:25.422+0000] {processor.py:161} INFO - Started process (PID=6018) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:05:25.423+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:05:25.427+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:05:25.426+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:05:25.451+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:05:25.447+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'bigquery_default', 'google_cloud_storage_conn_id': 'bigquery_default'}
[2024-02-20T19:05:25.451+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:05:25.511+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.095 seconds
[2024-02-20T19:05:27.221+0000] {processor.py:161} INFO - Started process (PID=6029) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:05:27.223+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:05:27.229+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:05:27.228+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:05:27.278+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:05:27.271+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentions'}
[2024-02-20T19:05:27.279+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:05:27.336+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.122 seconds
[2024-02-20T19:05:53.833+0000] {processor.py:161} INFO - Started process (PID=6075) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:05:53.835+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:05:53.839+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:05:53.838+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:05:53.873+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:05:53.869+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:05:53.874+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:05:53.942+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.114 seconds
[2024-02-20T19:06:24.002+0000] {processor.py:161} INFO - Started process (PID=6132) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:06:24.003+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:06:24.007+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:06:24.006+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:06:24.034+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:06:24.029+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:06:24.034+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:06:24.056+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.058 seconds
[2024-02-20T19:06:54.104+0000] {processor.py:161} INFO - Started process (PID=6189) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:06:54.106+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:06:54.110+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:06:54.110+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:06:54.139+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:06:54.134+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:06:54.140+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:06:54.203+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.105 seconds
[2024-02-20T19:07:24.943+0000] {processor.py:161} INFO - Started process (PID=6243) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:07:24.944+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:07:24.948+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:07:24.947+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:07:24.970+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:07:24.966+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:07:24.971+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:07:25.039+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.101 seconds
[2024-02-20T19:07:55.323+0000] {processor.py:161} INFO - Started process (PID=6300) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:07:55.324+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:07:55.327+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:07:55.327+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:07:55.358+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:07:55.352+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:07:55.358+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:07:55.400+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.083 seconds
[2024-02-20T19:08:26.265+0000] {processor.py:161} INFO - Started process (PID=6358) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:08:26.267+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:08:26.271+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:08:26.270+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:08:26.296+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:08:26.291+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:08:26.297+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:08:26.353+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.094 seconds
[2024-02-20T19:08:57.038+0000] {processor.py:161} INFO - Started process (PID=6415) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:08:57.040+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:08:57.044+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:08:57.043+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:08:57.066+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:08:57.062+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:08:57.066+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:08:57.090+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.060 seconds
[2024-02-20T19:09:27.301+0000] {processor.py:161} INFO - Started process (PID=6472) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:09:27.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:09:27.306+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:09:27.306+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:09:27.330+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:09:27.325+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:09:27.330+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:09:27.389+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.094 seconds
[2024-02-20T19:09:57.558+0000] {processor.py:161} INFO - Started process (PID=6529) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:09:57.559+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:09:57.562+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:09:57.562+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:09:57.585+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:09:57.581+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:09:57.586+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:09:57.645+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.093 seconds
[2024-02-20T19:10:28.045+0000] {processor.py:161} INFO - Started process (PID=6586) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:10:28.046+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:10:28.049+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:10:28.049+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:10:28.071+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:10:28.067+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:10:28.072+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:10:28.129+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.089 seconds
[2024-02-20T19:26:18.439+0000] {processor.py:161} INFO - Started process (PID=6638) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:26:18.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:26:18.447+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:26:18.446+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:26:18.517+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:26:18.493+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:26:18.518+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:26:18.564+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.149 seconds
[2024-02-20T19:26:48.671+0000] {processor.py:161} INFO - Started process (PID=6695) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:26:48.673+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:26:48.678+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:26:48.677+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:26:48.709+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:26:48.703+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:26:48.710+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:26:48.733+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.067 seconds
[2024-02-20T19:27:19.623+0000] {processor.py:161} INFO - Started process (PID=6752) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:27:19.624+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:27:19.626+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:27:19.625+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:27:19.639+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:27:19.636+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:27:19.639+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:27:19.654+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.034 seconds
[2024-02-20T19:27:50.480+0000] {processor.py:161} INFO - Started process (PID=6810) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:27:50.481+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:27:50.485+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:27:50.484+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:27:50.506+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:27:50.501+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:27:50.507+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:27:50.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.056 seconds
[2024-02-20T19:28:21.375+0000] {processor.py:161} INFO - Started process (PID=6867) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:28:21.376+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:28:21.379+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:28:21.378+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:28:21.398+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:28:21.394+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:28:21.398+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:28:21.418+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T19:28:51.766+0000] {processor.py:161} INFO - Started process (PID=6924) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:28:51.767+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:28:51.770+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:28:51.769+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:28:51.787+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:28:51.784+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:28:51.787+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:28:51.806+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.044 seconds
[2024-02-20T19:29:22.235+0000] {processor.py:161} INFO - Started process (PID=6981) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:29:22.236+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:29:22.240+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:29:22.239+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:29:22.260+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:29:22.255+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:29:22.261+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:29:22.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.051 seconds
[2024-02-20T19:29:52.605+0000] {processor.py:161} INFO - Started process (PID=7038) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:29:52.606+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:29:52.608+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:29:52.608+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:29:52.623+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:29:52.620+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:29:52.623+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:29:52.640+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.038 seconds
[2024-02-20T19:30:22.855+0000] {processor.py:161} INFO - Started process (PID=7095) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:30:22.856+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:30:22.859+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:30:22.859+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:30:22.874+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:30:22.871+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:30:22.874+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:30:22.890+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.038 seconds
[2024-02-20T19:30:53.039+0000] {processor.py:161} INFO - Started process (PID=7152) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:30:53.039+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:30:53.042+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:30:53.041+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:30:53.057+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:30:53.054+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:30:53.057+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:30:53.075+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.039 seconds
[2024-02-20T19:31:23.118+0000] {processor.py:161} INFO - Started process (PID=7209) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:31:23.119+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:31:23.122+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:31:23.122+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:31:23.139+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:31:23.136+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:31:23.139+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:31:23.158+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.042 seconds
[2024-02-20T19:31:53.412+0000] {processor.py:161} INFO - Started process (PID=7266) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:31:53.413+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:31:53.418+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:31:53.417+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:31:53.435+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:31:53.432+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:31:53.436+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:31:53.455+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T19:32:24.239+0000] {processor.py:161} INFO - Started process (PID=7323) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:32:24.240+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:32:24.243+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:32:24.243+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:32:24.258+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:32:24.255+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:32:24.259+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:32:24.279+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.042 seconds
[2024-02-20T19:32:54.412+0000] {processor.py:161} INFO - Started process (PID=7380) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:32:54.414+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:32:54.418+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:32:54.418+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:32:54.433+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:32:54.431+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:32:54.434+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:32:54.449+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T19:33:24.556+0000] {processor.py:161} INFO - Started process (PID=7437) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:33:24.556+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:33:24.559+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:33:24.559+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:33:24.572+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:33:24.570+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:33:24.573+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:33:24.597+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.044 seconds
[2024-02-20T19:33:54.807+0000] {processor.py:161} INFO - Started process (PID=7494) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:33:54.808+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:33:54.810+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:33:54.810+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:33:54.824+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:33:54.822+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:33:54.825+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:33:54.838+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.035 seconds
[2024-02-20T19:34:25.301+0000] {processor.py:161} INFO - Started process (PID=7551) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:34:25.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:34:25.304+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:34:25.304+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:34:25.318+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:34:25.315+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:34:25.318+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:34:25.332+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.034 seconds
[2024-02-20T19:34:56.129+0000] {processor.py:161} INFO - Started process (PID=7608) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:34:56.129+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:34:56.132+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:34:56.132+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:34:56.146+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:34:56.143+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:34:56.146+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:34:56.160+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.034 seconds
[2024-02-20T19:35:26.763+0000] {processor.py:161} INFO - Started process (PID=7665) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:35:26.764+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:35:26.766+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:35:26.766+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:35:26.780+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:35:26.777+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:35:26.780+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:35:26.794+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.033 seconds
[2024-02-20T19:35:57.344+0000] {processor.py:161} INFO - Started process (PID=7722) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:35:57.345+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:35:57.348+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:35:57.348+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:35:57.363+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:35:57.360+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:35:57.363+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:35:57.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.037 seconds
[2024-02-20T19:36:28.097+0000] {processor.py:161} INFO - Started process (PID=7779) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:36:28.097+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:36:28.100+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:36:28.099+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:36:28.113+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:36:28.111+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:36:28.114+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:36:28.128+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.034 seconds
[2024-02-20T19:36:58.643+0000] {processor.py:161} INFO - Started process (PID=7836) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:36:58.644+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:36:58.647+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:36:58.646+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:36:58.660+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:36:58.657+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:36:58.660+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:36:58.674+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.034 seconds
[2024-02-20T19:37:29.271+0000] {processor.py:161} INFO - Started process (PID=7893) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:37:29.272+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:37:29.277+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:37:29.277+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:37:29.295+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:37:29.292+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:37:29.295+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:37:29.312+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.044 seconds
[2024-02-20T19:37:59.920+0000] {processor.py:161} INFO - Started process (PID=7950) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:37:59.921+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:37:59.923+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:37:59.923+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:37:59.939+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:37:59.936+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:37:59.939+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:37:59.955+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.037 seconds
[2024-02-20T19:38:30.668+0000] {processor.py:161} INFO - Started process (PID=8007) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:38:30.669+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:38:30.671+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:38:30.671+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:38:30.685+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:38:30.682+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:38:30.685+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:38:30.710+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.044 seconds
[2024-02-20T19:39:01.355+0000] {processor.py:161} INFO - Started process (PID=8064) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:39:01.356+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:39:01.358+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:39:01.358+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:39:01.372+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:39:01.369+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:39:01.372+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:39:01.387+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.035 seconds
[2024-02-20T19:39:32.031+0000] {processor.py:161} INFO - Started process (PID=8121) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:39:32.032+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:39:32.034+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:39:32.034+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:39:32.046+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:39:32.044+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:39:32.047+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:39:32.068+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.040 seconds
[2024-02-20T19:40:02.633+0000] {processor.py:161} INFO - Started process (PID=8178) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:40:02.634+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:40:02.636+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:40:02.635+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:40:02.651+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:40:02.648+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:40:02.651+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:40:02.665+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.034 seconds
[2024-02-20T19:40:33.298+0000] {processor.py:161} INFO - Started process (PID=8235) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:40:33.299+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:40:33.301+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:40:33.301+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:40:33.313+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:40:33.310+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:40:33.313+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:40:33.336+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.041 seconds
[2024-02-20T19:41:03.972+0000] {processor.py:161} INFO - Started process (PID=8292) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:41:03.973+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:41:03.975+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:41:03.974+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:41:03.988+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:41:03.985+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:41:03.988+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:41:04.008+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.038 seconds
[2024-02-20T19:41:34.605+0000] {processor.py:161} INFO - Started process (PID=8349) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:41:34.606+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:41:34.608+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:41:34.608+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:41:34.621+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:41:34.619+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:41:34.622+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:41:34.645+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.043 seconds
[2024-02-20T19:42:04.947+0000] {processor.py:161} INFO - Started process (PID=8406) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:42:04.949+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:42:04.952+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:42:04.951+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:42:04.968+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:42:04.965+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:42:04.969+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:42:04.992+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.055 seconds
[2024-02-20T19:42:35.710+0000] {processor.py:161} INFO - Started process (PID=8463) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:42:35.711+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:42:35.713+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:42:35.713+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:42:35.727+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:42:35.724+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:42:35.727+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:42:35.742+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.035 seconds
[2024-02-20T19:43:06.435+0000] {processor.py:161} INFO - Started process (PID=8520) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:43:06.435+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:43:06.438+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:43:06.438+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:43:06.451+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:43:06.448+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:43:06.451+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:43:06.474+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.042 seconds
[2024-02-20T19:43:37.150+0000] {processor.py:161} INFO - Started process (PID=8577) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:43:37.151+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:43:37.153+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:43:37.153+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:43:37.167+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:43:37.164+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:43:37.167+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:43:37.181+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.033 seconds
[2024-02-20T19:44:07.855+0000] {processor.py:161} INFO - Started process (PID=8634) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:44:07.856+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:44:07.858+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:44:07.858+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:44:07.874+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:44:07.871+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:44:07.874+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:44:07.892+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.040 seconds
[2024-02-20T19:44:38.481+0000] {processor.py:161} INFO - Started process (PID=8691) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:44:38.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:44:38.485+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:44:38.484+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:44:38.498+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:44:38.495+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:44:38.498+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:44:38.512+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.033 seconds
[2024-02-20T19:45:09.282+0000] {processor.py:161} INFO - Started process (PID=8748) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:45:09.282+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:45:09.285+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:45:09.285+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:45:09.298+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:45:09.296+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:45:09.299+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:45:09.323+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.044 seconds
[2024-02-20T19:45:40.238+0000] {processor.py:161} INFO - Started process (PID=8805) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:45:40.239+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:45:40.241+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:45:40.241+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:45:40.255+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:45:40.252+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:45:40.255+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:45:40.269+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.034 seconds
[2024-02-20T19:46:10.996+0000] {processor.py:161} INFO - Started process (PID=8862) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:46:10.997+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:46:10.999+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:46:10.999+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:46:11.014+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:46:11.010+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:46:11.014+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:46:11.029+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.035 seconds
[2024-02-20T19:46:41.870+0000] {processor.py:161} INFO - Started process (PID=8919) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:46:41.871+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:46:41.874+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:46:41.873+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:46:41.888+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:46:41.885+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:46:41.888+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:46:41.907+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.040 seconds
[2024-02-20T19:47:12.836+0000] {processor.py:161} INFO - Started process (PID=8976) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:47:12.837+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:47:12.840+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:47:12.839+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:47:12.853+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:47:12.851+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:47:12.854+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:47:12.868+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.034 seconds
[2024-02-20T19:47:43.283+0000] {processor.py:161} INFO - Started process (PID=9033) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:47:43.284+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:47:43.287+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:47:43.287+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:47:43.302+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:47:43.299+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:47:43.302+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:47:43.316+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.036 seconds
[2024-02-20T19:48:14.216+0000] {processor.py:161} INFO - Started process (PID=9090) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:48:14.217+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:48:14.219+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:48:14.219+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:48:14.232+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:48:14.230+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:48:14.233+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:48:14.256+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.043 seconds
[2024-02-20T19:48:44.492+0000] {processor.py:161} INFO - Started process (PID=9147) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:48:44.493+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:48:44.495+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:48:44.495+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:48:44.509+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:48:44.506+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:48:44.509+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:48:44.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.045 seconds
[2024-02-20T19:49:15.309+0000] {processor.py:161} INFO - Started process (PID=9204) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:49:15.309+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:49:15.312+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:49:15.312+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:49:15.326+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:49:15.323+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:49:15.326+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:49:15.341+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.035 seconds
[2024-02-20T19:49:45.988+0000] {processor.py:161} INFO - Started process (PID=9261) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:49:45.989+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:49:45.991+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:49:45.991+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:49:46.005+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:49:46.003+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:49:46.006+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:49:46.029+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.044 seconds
[2024-02-20T19:50:16.490+0000] {processor.py:161} INFO - Started process (PID=9318) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:50:16.491+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:50:16.497+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:50:16.497+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:50:16.511+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:50:16.508+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:50:16.511+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:50:16.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.039 seconds
[2024-02-20T19:50:47.497+0000] {processor.py:161} INFO - Started process (PID=9375) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:50:47.498+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:50:47.500+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:50:47.500+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:50:47.518+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:50:47.515+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:50:47.518+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:50:47.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.051 seconds
[2024-02-20T19:51:18.218+0000] {processor.py:161} INFO - Started process (PID=9433) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:51:18.219+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:51:18.222+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:51:18.221+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:51:18.237+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:51:18.234+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:51:18.237+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:51:18.253+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.038 seconds
[2024-02-20T19:51:48.773+0000] {processor.py:161} INFO - Started process (PID=9490) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:51:48.775+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:51:48.779+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:51:48.779+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:51:48.796+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:51:48.793+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:51:48.797+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:51:48.822+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.052 seconds
[2024-02-20T19:52:19.423+0000] {processor.py:161} INFO - Started process (PID=9547) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:52:19.424+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:52:19.426+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:52:19.426+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:52:19.441+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:52:19.438+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:52:19.441+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:52:19.457+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.038 seconds
[2024-02-20T19:52:50.203+0000] {processor.py:161} INFO - Started process (PID=9605) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:52:50.204+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:52:50.207+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:52:50.207+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:52:50.220+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:52:50.218+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:52:50.220+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:52:50.234+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.034 seconds
[2024-02-20T19:53:20.769+0000] {processor.py:161} INFO - Started process (PID=9668) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:53:20.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:53:20.773+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:53:20.773+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:53:20.796+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:53:20.792+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:53:20.797+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:53:20.831+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.066 seconds
[2024-02-20T19:53:51.368+0000] {processor.py:161} INFO - Started process (PID=9725) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:53:51.368+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:53:51.371+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:53:51.371+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:53:51.385+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:53:51.383+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:53:51.386+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:53:51.411+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T19:54:21.798+0000] {processor.py:161} INFO - Started process (PID=9782) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:54:21.798+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:54:21.801+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:54:21.801+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:54:21.815+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:54:21.812+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:54:21.815+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:54:21.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.045 seconds
[2024-02-20T19:54:52.502+0000] {processor.py:161} INFO - Started process (PID=9839) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:54:52.502+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:54:52.505+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:54:52.505+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:54:52.519+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:54:52.517+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:54:52.520+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:54:52.538+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.039 seconds
[2024-02-20T19:55:23.037+0000] {processor.py:161} INFO - Started process (PID=9896) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:55:23.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:55:23.040+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:55:23.040+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:55:23.054+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:55:23.052+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:55:23.055+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:55:23.069+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.035 seconds
[2024-02-20T19:55:53.889+0000] {processor.py:161} INFO - Started process (PID=9953) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:55:53.890+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:55:53.893+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:55:53.892+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:55:53.907+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:55:53.904+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:55:53.908+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:55:53.932+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T19:56:24.831+0000] {processor.py:161} INFO - Started process (PID=10010) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:56:24.832+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:56:24.835+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:56:24.835+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:56:24.850+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:56:24.847+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:56:24.850+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:56:24.864+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.036 seconds
[2024-02-20T19:56:55.533+0000] {processor.py:161} INFO - Started process (PID=10067) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:56:55.534+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:56:55.537+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:56:55.537+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:56:55.552+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:56:55.549+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:56:55.552+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:56:55.577+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.047 seconds
[2024-02-20T19:57:26.252+0000] {processor.py:161} INFO - Started process (PID=10124) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:57:26.253+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:57:26.256+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:57:26.256+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:57:26.271+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:57:26.268+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:57:26.271+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:57:26.286+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.037 seconds
[2024-02-20T19:57:57.255+0000] {processor.py:161} INFO - Started process (PID=10181) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:57:57.255+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:57:57.258+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:57:57.258+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:57:57.275+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:57:57.272+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:57:57.276+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:57:57.300+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.049 seconds
[2024-02-20T19:58:28.061+0000] {processor.py:161} INFO - Started process (PID=10238) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:58:28.062+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:58:28.064+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:58:28.064+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:58:28.082+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:58:28.077+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:58:28.083+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:58:28.116+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.058 seconds
[2024-02-20T19:58:58.814+0000] {processor.py:161} INFO - Started process (PID=10295) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:58:58.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:58:58.817+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:58:58.817+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:58:58.832+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:58:58.829+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:58:58.832+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:58:58.846+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.036 seconds
[2024-02-20T19:59:29.307+0000] {processor.py:161} INFO - Started process (PID=10352) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:59:29.308+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:59:29.311+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:59:29.310+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:59:29.326+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:59:29.323+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:59:29.326+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:59:29.340+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.037 seconds
[2024-02-20T19:59:59.872+0000] {processor.py:161} INFO - Started process (PID=10409) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T19:59:59.873+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T19:59:59.875+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:59:59.875+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T19:59:59.890+0000] {logging_mixin.py:188} INFO - [2024-02-20T19:59:59.887+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T19:59:59.890+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T19:59:59.914+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.045 seconds
[2024-02-20T20:00:27.506+0000] {processor.py:161} INFO - Started process (PID=10466) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:00:27.507+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:00:27.509+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:00:27.509+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:00:27.528+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:00:27.525+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:00:27.528+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:00:27.552+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.049 seconds
[2024-02-20T20:00:58.370+0000] {processor.py:161} INFO - Started process (PID=10523) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:00:58.371+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:00:58.373+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:00:58.373+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:00:58.386+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:00:58.384+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:00:58.387+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:00:58.411+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.044 seconds
[2024-02-20T20:01:29.168+0000] {processor.py:161} INFO - Started process (PID=10581) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:01:29.169+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:01:29.171+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:01:29.171+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:01:29.186+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:01:29.183+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:01:29.186+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:01:29.211+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T20:01:59.752+0000] {processor.py:161} INFO - Started process (PID=10639) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:01:59.753+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:01:59.755+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:01:59.755+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:01:59.770+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:01:59.767+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:01:59.770+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:01:59.795+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T20:02:30.336+0000] {processor.py:161} INFO - Started process (PID=10697) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:02:30.337+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:02:30.339+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:02:30.339+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:02:30.353+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:02:30.351+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:02:30.354+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:02:30.369+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.036 seconds
[2024-02-20T20:03:01.077+0000] {processor.py:161} INFO - Started process (PID=10755) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:03:01.078+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:03:01.081+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:03:01.081+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:03:01.097+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:03:01.094+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:03:01.097+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:03:01.116+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.044 seconds
[2024-02-20T20:03:31.617+0000] {processor.py:161} INFO - Started process (PID=10814) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:03:31.618+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:03:31.620+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:03:31.619+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:03:31.635+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:03:31.632+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:03:31.636+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:03:31.661+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.047 seconds
[2024-02-20T20:04:02.278+0000] {processor.py:161} INFO - Started process (PID=10872) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:04:02.279+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:04:02.281+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:04:02.281+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:04:02.298+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:04:02.294+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:04:02.298+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:04:02.330+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.056 seconds
[2024-02-20T20:04:32.982+0000] {processor.py:161} INFO - Started process (PID=10930) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:04:32.983+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:04:32.985+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:04:32.985+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:04:32.999+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:04:32.997+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:04:33.000+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:04:33.014+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.034 seconds
[2024-02-20T20:05:03.549+0000] {processor.py:161} INFO - Started process (PID=10988) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:05:03.550+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:05:03.552+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:05:03.552+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:05:03.566+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:05:03.563+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:05:03.566+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:05:03.581+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.035 seconds
[2024-02-20T20:05:34.163+0000] {processor.py:161} INFO - Started process (PID=11046) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:05:34.164+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:05:34.166+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:05:34.166+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:05:34.181+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:05:34.178+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:05:34.182+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:05:34.196+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.036 seconds
[2024-02-20T20:06:05.058+0000] {processor.py:161} INFO - Started process (PID=11104) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:06:05.058+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:06:05.060+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:06:05.060+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:06:05.075+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:06:05.072+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:06:05.075+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:06:05.099+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.045 seconds
[2024-02-20T20:06:35.819+0000] {processor.py:161} INFO - Started process (PID=11162) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:06:35.820+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:06:35.822+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:06:35.821+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:06:35.836+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:06:35.833+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:06:35.836+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:06:35.851+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.036 seconds
[2024-02-20T20:07:06.471+0000] {processor.py:161} INFO - Started process (PID=11220) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:07:06.472+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:07:06.475+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:07:06.475+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:07:06.492+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:07:06.490+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:07:06.493+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:07:06.510+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.044 seconds
[2024-02-20T20:07:37.317+0000] {processor.py:161} INFO - Started process (PID=11278) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:07:37.317+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:07:37.320+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:07:37.319+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:07:37.334+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:07:37.331+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:07:37.334+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:07:37.353+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.039 seconds
[2024-02-20T20:08:07.936+0000] {processor.py:161} INFO - Started process (PID=11336) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:08:07.937+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:08:07.939+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:08:07.939+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:08:07.953+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:08:07.950+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:08:07.953+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:08:07.978+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.045 seconds
[2024-02-20T20:08:38.642+0000] {processor.py:161} INFO - Started process (PID=11394) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:08:38.642+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:08:38.645+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:08:38.644+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:08:38.658+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:08:38.656+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:08:38.659+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:08:38.684+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.045 seconds
[2024-02-20T20:09:09.177+0000] {processor.py:161} INFO - Started process (PID=11452) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:09:09.178+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:09:09.180+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:09:09.180+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:09:09.194+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:09:09.191+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:09:09.194+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:09:09.210+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.036 seconds
[2024-02-20T20:09:39.953+0000] {processor.py:161} INFO - Started process (PID=11510) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:09:39.953+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:09:39.956+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:09:39.955+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:09:39.970+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:09:39.967+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:09:39.971+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:09:39.987+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.038 seconds
[2024-02-20T20:10:10.647+0000] {processor.py:161} INFO - Started process (PID=11568) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:10:10.648+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:10:10.650+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:10:10.650+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:10:10.665+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:10:10.661+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:10:10.665+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:10:10.692+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.048 seconds
[2024-02-20T20:10:41.412+0000] {processor.py:161} INFO - Started process (PID=11626) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:10:41.413+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:10:41.416+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:10:41.416+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:10:41.430+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:10:41.428+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:10:41.431+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:10:41.456+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.047 seconds
[2024-02-20T20:11:12.222+0000] {processor.py:161} INFO - Started process (PID=11684) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:11:12.223+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:11:12.225+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:11:12.225+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:11:12.240+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:11:12.237+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:11:12.240+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:11:12.265+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T20:11:42.912+0000] {processor.py:161} INFO - Started process (PID=11742) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:11:42.913+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:11:42.915+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:11:42.915+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:11:42.931+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:11:42.928+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:11:42.932+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:11:42.952+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.043 seconds
[2024-02-20T20:12:13.708+0000] {processor.py:161} INFO - Started process (PID=11800) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:12:13.709+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:12:13.712+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:12:13.711+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:12:13.726+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:12:13.723+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:12:13.727+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:12:13.742+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.036 seconds
[2024-02-20T20:12:44.564+0000] {processor.py:161} INFO - Started process (PID=11858) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:12:44.565+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:12:44.567+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:12:44.567+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:12:44.580+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:12:44.577+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:12:44.580+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:12:44.595+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.034 seconds
[2024-02-20T20:13:15.396+0000] {processor.py:161} INFO - Started process (PID=11916) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:13:15.397+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:13:15.399+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:13:15.399+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:13:15.414+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:13:15.411+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:13:15.414+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:13:15.439+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T20:13:46.210+0000] {processor.py:161} INFO - Started process (PID=11974) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:13:46.211+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:13:46.213+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:13:46.213+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:13:46.227+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:13:46.224+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:13:46.227+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:13:46.254+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.047 seconds
[2024-02-20T20:14:17.082+0000] {processor.py:161} INFO - Started process (PID=12032) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:14:17.082+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:14:17.085+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:14:17.084+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:14:17.099+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:14:17.096+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:14:17.099+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:14:17.126+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.048 seconds
[2024-02-20T20:14:47.914+0000] {processor.py:161} INFO - Started process (PID=12090) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:14:47.915+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:14:47.918+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:14:47.918+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:14:47.933+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:14:47.931+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:14:47.934+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:14:47.949+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.038 seconds
[2024-02-20T20:15:18.565+0000] {processor.py:161} INFO - Started process (PID=12149) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:15:18.566+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:15:18.568+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:15:18.568+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:15:18.583+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:15:18.580+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:15:18.583+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:15:18.611+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.050 seconds
[2024-02-20T20:15:49.318+0000] {processor.py:161} INFO - Started process (PID=12207) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:15:49.319+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:15:49.321+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:15:49.320+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:15:49.335+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:15:49.332+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:15:49.335+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:15:49.356+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.041 seconds
[2024-02-20T20:16:19.951+0000] {processor.py:161} INFO - Started process (PID=12265) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:16:19.952+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:16:19.955+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:16:19.954+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:16:19.969+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:16:19.966+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'credentials'}
[2024-02-20T20:16:19.970+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:16:20.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.064 seconds
[2024-02-20T20:16:23.095+0000] {processor.py:161} INFO - Started process (PID=12272) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:16:23.096+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:16:23.100+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:16:23.100+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:16:23.128+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:16:23.124+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:16:23.128+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:16:23.145+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.053 seconds
[2024-02-20T20:16:53.798+0000] {processor.py:161} INFO - Started process (PID=12330) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:16:53.799+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:16:53.802+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:16:53.801+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:16:53.817+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:16:53.814+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:16:53.817+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:16:53.841+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T20:17:24.453+0000] {processor.py:161} INFO - Started process (PID=12388) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:17:24.453+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:17:24.456+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:17:24.456+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:17:24.470+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:17:24.467+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:17:24.470+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:17:24.504+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.054 seconds
[2024-02-20T20:17:32.190+0000] {processor.py:161} INFO - Started process (PID=12420) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:17:32.190+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:17:32.193+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:17:32.193+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:17:32.213+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:17:32.210+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:17:32.213+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:17:32.234+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.048 seconds
[2024-02-20T20:18:02.864+0000] {processor.py:161} INFO - Started process (PID=12478) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:18:02.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:18:02.868+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:18:02.868+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:18:02.883+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:18:02.880+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:18:02.883+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:18:02.899+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.037 seconds
[2024-02-20T20:18:33.537+0000] {processor.py:161} INFO - Started process (PID=12536) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:18:33.538+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:18:33.540+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:18:33.540+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:18:33.554+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:18:33.551+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:18:33.555+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:18:33.581+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T20:19:04.146+0000] {processor.py:161} INFO - Started process (PID=12594) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:19:04.147+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:19:04.149+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:19:04.149+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:19:04.163+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:19:04.160+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:19:04.163+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:19:04.183+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.040 seconds
[2024-02-20T20:19:34.906+0000] {processor.py:161} INFO - Started process (PID=12652) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:19:34.907+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:19:34.910+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:19:34.909+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:19:34.924+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:19:34.921+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:19:34.925+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:19:34.956+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.053 seconds
[2024-02-20T20:20:05.720+0000] {processor.py:161} INFO - Started process (PID=12710) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:20:05.721+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:20:05.723+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:20:05.722+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:20:05.737+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:20:05.735+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:20:05.738+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:20:05.765+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.048 seconds
[2024-02-20T20:20:36.551+0000] {processor.py:161} INFO - Started process (PID=12768) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:20:36.553+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:20:36.555+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:20:36.555+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:20:36.570+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:20:36.567+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:20:36.570+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:20:36.587+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.038 seconds
[2024-02-20T20:21:07.286+0000] {processor.py:161} INFO - Started process (PID=12826) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:21:07.287+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:21:07.289+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:21:07.289+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:21:07.304+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:21:07.301+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:21:07.304+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:21:07.330+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.047 seconds
[2024-02-20T20:21:38.048+0000] {processor.py:161} INFO - Started process (PID=12884) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:21:38.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:21:38.052+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:21:38.052+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:21:38.067+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:21:38.064+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:21:38.068+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:21:38.087+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.042 seconds
[2024-02-20T20:22:08.690+0000] {processor.py:161} INFO - Started process (PID=12942) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:22:08.691+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:22:08.694+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:22:08.693+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:22:08.708+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:22:08.705+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:22:08.709+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:22:08.725+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.038 seconds
[2024-02-20T20:22:39.377+0000] {processor.py:161} INFO - Started process (PID=13000) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:22:39.379+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:22:39.383+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:22:39.383+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:22:39.411+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:22:39.407+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:22:39.420+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:22:39.450+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.086 seconds
[2024-02-20T20:23:10.270+0000] {processor.py:161} INFO - Started process (PID=13058) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:23:10.271+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:23:10.274+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:23:10.273+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:23:10.288+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:23:10.285+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:23:10.288+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:23:10.313+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T20:23:40.955+0000] {processor.py:161} INFO - Started process (PID=13116) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:23:40.956+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:23:40.958+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:23:40.958+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:23:40.973+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:23:40.970+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:23:40.973+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:23:40.990+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.038 seconds
[2024-02-20T20:24:11.553+0000] {processor.py:161} INFO - Started process (PID=13174) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:24:11.554+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:24:11.556+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:24:11.556+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:24:11.570+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:24:11.567+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:24:11.570+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:24:11.587+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.037 seconds
[2024-02-20T20:24:42.170+0000] {processor.py:161} INFO - Started process (PID=13232) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:24:42.171+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:24:42.174+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:24:42.173+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:24:42.191+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:24:42.188+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:24:42.192+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:24:42.210+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.043 seconds
[2024-02-20T20:25:12.583+0000] {processor.py:161} INFO - Started process (PID=13290) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:25:12.584+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:25:12.586+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:25:12.586+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:25:12.601+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:25:12.598+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:25:12.602+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:25:12.621+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.041 seconds
[2024-02-20T20:25:42.717+0000] {processor.py:161} INFO - Started process (PID=13348) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:25:42.718+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:25:42.720+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:25:42.720+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:25:42.734+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:25:42.732+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:25:42.735+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:25:42.749+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.036 seconds
[2024-02-20T20:26:12.787+0000] {processor.py:161} INFO - Started process (PID=13404) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:26:12.788+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:26:12.790+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:26:12.790+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:26:12.805+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:26:12.802+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:26:12.805+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:26:12.820+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.036 seconds
[2024-02-20T20:26:42.936+0000] {processor.py:161} INFO - Started process (PID=13462) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:26:42.937+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:26:42.939+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:26:42.939+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:26:42.954+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:26:42.952+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:26:42.955+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:26:42.971+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.039 seconds
[2024-02-20T20:27:13.816+0000] {processor.py:161} INFO - Started process (PID=13517) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:27:13.817+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:27:13.819+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:27:13.819+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:27:13.834+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:27:13.831+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:27:13.835+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:27:13.850+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.037 seconds
[2024-02-20T20:27:43.977+0000] {processor.py:161} INFO - Started process (PID=13575) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:27:43.978+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:27:43.980+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:27:43.979+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:27:43.994+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:27:43.991+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:27:43.995+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:27:44.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.037 seconds
[2024-02-20T20:28:14.825+0000] {processor.py:161} INFO - Started process (PID=13633) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:28:14.825+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:28:14.828+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:28:14.827+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:28:14.842+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:28:14.839+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'GBQ-cred', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:28:14.843+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:28:14.859+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.037 seconds
[2024-02-20T20:28:34.162+0000] {processor.py:161} INFO - Started process (PID=13665) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:28:34.163+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:28:34.166+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:28:34.165+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:28:34.185+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:28:34.183+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'google-cloud', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:28:34.186+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:28:34.200+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.043 seconds
[2024-02-20T20:29:04.371+0000] {processor.py:161} INFO - Started process (PID=13723) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:29:04.372+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:29:04.375+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:29:04.374+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:29:04.389+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:29:04.386+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'google-cloud', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:29:04.390+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:29:04.404+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.037 seconds
[2024-02-20T20:29:35.088+0000] {processor.py:161} INFO - Started process (PID=13781) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:29:35.089+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:29:35.091+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:29:35.091+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:29:35.107+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:29:35.103+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'google-cloud', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:29:35.108+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:29:35.136+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.051 seconds
[2024-02-20T20:30:05.911+0000] {processor.py:161} INFO - Started process (PID=13839) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:30:05.911+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:30:05.914+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:30:05.913+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:30:05.928+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:30:05.925+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'google-cloud', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:30:05.928+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:30:05.954+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.047 seconds
[2024-02-20T20:30:36.759+0000] {processor.py:161} INFO - Started process (PID=13897) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:30:36.760+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:30:36.762+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:30:36.762+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:30:36.776+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:30:36.774+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'google-cloud', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:30:36.777+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:30:36.801+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.045 seconds
[2024-02-20T20:31:07.520+0000] {processor.py:161} INFO - Started process (PID=13955) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:31:07.520+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:31:07.522+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:31:07.522+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:31:07.537+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:31:07.534+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'google-cloud', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:31:07.537+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:31:07.552+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.036 seconds
[2024-02-20T20:31:38.207+0000] {processor.py:161} INFO - Started process (PID=14013) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:31:38.208+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:31:38.210+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:31:38.210+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:31:38.225+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:31:38.222+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'google-cloud', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:31:38.225+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:31:38.242+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.038 seconds
[2024-02-20T20:32:09.184+0000] {processor.py:161} INFO - Started process (PID=14071) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:32:09.185+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:32:09.187+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:32:09.186+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:32:09.202+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:32:09.199+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'google-cloud', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:32:09.203+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:32:09.227+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T20:32:39.363+0000] {processor.py:161} INFO - Started process (PID=14129) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:32:39.364+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:32:39.366+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:32:39.366+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:32:39.381+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:32:39.378+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'google-cloud', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:32:39.381+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:32:39.396+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.037 seconds
[2024-02-20T20:33:09.665+0000] {processor.py:161} INFO - Started process (PID=14187) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:33:09.666+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:33:09.668+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:33:09.668+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:33:09.682+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:33:09.679+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'bigquery_conn_id': 'google-cloud', 'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:33:09.682+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:33:09.707+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.046 seconds
[2024-02-20T20:33:27.463+0000] {processor.py:161} INFO - Started process (PID=14244) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:33:27.464+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:33:27.466+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:33:27.466+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:33:27.489+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:33:27.487+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:33:27.490+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:33:27.509+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.049 seconds
[2024-02-20T20:33:58.280+0000] {processor.py:161} INFO - Started process (PID=14302) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:33:58.281+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:33:58.283+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:33:58.282+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:33:58.297+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:33:58.295+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:33:58.298+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:33:58.313+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.036 seconds
[2024-02-20T20:34:14.445+0000] {processor.py:161} INFO - Started process (PID=14304) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:34:14.446+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:34:14.448+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:34:14.448+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:34:14.467+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:34:14.464+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:34:14.468+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:34:14.483+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.041 seconds
[2024-02-20T20:34:31.126+0000] {processor.py:161} INFO - Started process (PID=14360) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:34:31.127+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:34:31.129+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:34:31.129+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:34:31.153+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:34:31.255+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:34:31.255+0000] {override.py:1769} INFO - Created Permission View: can read on DAG:processing_sales
[2024-02-20T20:34:31.264+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:34:31.264+0000] {override.py:1769} INFO - Created Permission View: can edit on DAG:processing_sales
[2024-02-20T20:34:31.270+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:34:31.270+0000] {override.py:1769} INFO - Created Permission View: can delete on DAG:processing_sales
[2024-02-20T20:34:31.271+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:34:31.270+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:34:31.282+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:34:31.282+0000] {dag.py:3055} INFO - Creating ORM DAG for processing_sales
[2024-02-20T20:34:31.291+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:34:31.291+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:34:31.308+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.185 seconds
[2024-02-20T20:35:02.131+0000] {processor.py:161} INFO - Started process (PID=14417) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:35:02.132+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:35:02.135+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:35:02.135+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:35:02.155+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:35:02.172+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:35:02.172+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:35:02.192+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:35:02.192+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:35:02.219+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.090 seconds
[2024-02-20T20:35:32.319+0000] {processor.py:161} INFO - Started process (PID=14475) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:35:32.320+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:35:32.322+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:35:32.322+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:35:32.338+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:35:32.355+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:35:32.355+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:35:32.378+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:35:32.377+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:35:32.395+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.079 seconds
[2024-02-20T20:36:02.587+0000] {processor.py:161} INFO - Started process (PID=14532) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:36:02.589+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:36:02.589+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:36:02.589+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:36:02.610+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:36:02.631+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:36:02.630+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:36:02.657+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:36:02.656+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:36:02.676+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.093 seconds
[2024-02-20T20:36:33.458+0000] {processor.py:161} INFO - Started process (PID=14589) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:36:33.459+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:36:33.460+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:36:33.460+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:36:33.478+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:36:33.495+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:36:33.494+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:36:33.517+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:36:33.516+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:36:33.542+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.087 seconds
[2024-02-20T20:37:03.741+0000] {processor.py:161} INFO - Started process (PID=14646) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:37:03.742+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:37:03.742+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:37:03.742+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:37:03.760+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:37:03.778+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:37:03.777+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:37:03.815+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.078 seconds
[2024-02-20T20:37:34.473+0000] {processor.py:161} INFO - Started process (PID=14703) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:37:34.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:37:34.475+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:37:34.474+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:37:34.493+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:37:34.510+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:37:34.509+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:37:34.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.075 seconds
[2024-02-20T20:38:05.015+0000] {processor.py:161} INFO - Started process (PID=14760) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:38:05.016+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:38:05.017+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:38:05.016+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:38:05.034+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:38:05.052+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:38:05.052+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:38:05.088+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.076 seconds
[2024-02-20T20:38:35.129+0000] {processor.py:161} INFO - Started process (PID=14817) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:38:35.130+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:38:35.131+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:38:35.131+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:38:35.151+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:38:35.173+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:38:35.173+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:38:35.210+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.086 seconds
[2024-02-20T20:39:05.336+0000] {processor.py:161} INFO - Started process (PID=14874) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:39:05.336+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:39:05.337+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:39:05.337+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:39:05.354+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:39:05.372+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:39:05.371+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:39:05.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.075 seconds
[2024-02-20T20:39:35.652+0000] {processor.py:161} INFO - Started process (PID=14931) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:39:35.653+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:39:35.654+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:39:35.654+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:39:35.672+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:39:35.690+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:39:35.690+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:39:35.727+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.078 seconds
[2024-02-20T20:40:05.881+0000] {processor.py:161} INFO - Started process (PID=14988) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:40:05.882+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:40:05.883+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:40:05.882+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:40:05.900+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:40:05.989+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:40:05.989+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:40:05.999+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:40:05.999+0000] {dag.py:3055} INFO - Creating ORM DAG for processing_sales
[2024-02-20T20:40:06.009+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:40:06.009+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:40:06.024+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.146 seconds
[2024-02-20T20:40:36.769+0000] {processor.py:161} INFO - Started process (PID=15044) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:40:36.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:40:36.773+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:40:36.772+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:40:36.799+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:40:36.826+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:40:36.825+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:40:36.859+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:40:36.859+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:40:36.882+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.117 seconds
[2024-02-20T20:41:07.894+0000] {processor.py:161} INFO - Started process (PID=15101) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:41:07.895+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:41:07.896+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:41:07.896+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:41:07.915+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:41:07.935+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:41:07.934+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:41:07.961+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:41:07.961+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:41:07.989+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.099 seconds
[2024-02-20T20:41:38.649+0000] {processor.py:161} INFO - Started process (PID=15158) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:41:38.650+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:41:38.651+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:41:38.651+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:41:38.671+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:41:38.692+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:41:38.691+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:41:38.716+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:41:38.715+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:41:38.743+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.098 seconds
[2024-02-20T20:42:09.340+0000] {processor.py:161} INFO - Started process (PID=15215) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:42:09.341+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:42:09.342+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:42:09.342+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:42:09.362+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:42:09.378+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:42:09.378+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:42:09.425+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.088 seconds
[2024-02-20T20:42:39.923+0000] {processor.py:161} INFO - Started process (PID=15272) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:42:39.924+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:42:39.925+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:42:39.924+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:42:39.942+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:42:39.960+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:42:39.959+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:42:40.004+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.085 seconds
[2024-02-20T20:43:10.536+0000] {processor.py:161} INFO - Started process (PID=15329) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:43:10.538+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:43:10.539+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:43:10.539+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:43:10.558+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:43:10.577+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:43:10.576+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:43:10.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.081 seconds
[2024-02-20T20:43:41.230+0000] {processor.py:161} INFO - Started process (PID=15386) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:43:41.231+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:43:41.231+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:43:41.231+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:43:41.248+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:43:41.266+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:43:41.266+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:43:41.313+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.087 seconds
[2024-02-20T20:44:11.857+0000] {processor.py:161} INFO - Started process (PID=15443) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:44:11.858+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:44:11.859+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:44:11.858+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:44:11.879+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:44:11.898+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:44:11.898+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:44:11.937+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.084 seconds
[2024-02-20T20:44:42.644+0000] {processor.py:161} INFO - Started process (PID=15500) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:44:42.645+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:44:42.647+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:44:42.646+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:44:42.665+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:44:42.685+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:44:42.685+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:44:42.733+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.091 seconds
[2024-02-20T20:45:13.530+0000] {processor.py:161} INFO - Started process (PID=15557) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:45:13.531+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:45:13.532+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:45:13.531+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:45:13.549+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:45:13.567+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:45:13.567+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:45:13.607+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.081 seconds
[2024-02-20T20:45:44.203+0000] {processor.py:161} INFO - Started process (PID=15614) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:45:44.204+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:45:44.204+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:45:44.204+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:45:44.223+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:45:44.241+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:45:44.240+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:45:44.281+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.081 seconds
[2024-02-20T20:46:14.884+0000] {processor.py:161} INFO - Started process (PID=15671) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:46:14.885+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:46:14.886+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:46:14.886+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:46:14.905+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:46:14.922+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:46:14.922+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:46:14.961+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.080 seconds
[2024-02-20T20:46:45.672+0000] {processor.py:161} INFO - Started process (PID=15728) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:46:45.673+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:46:45.674+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:46:45.674+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:46:45.693+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:46:45.711+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:46:45.711+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:46:45.756+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.087 seconds
[2024-02-20T20:47:16.383+0000] {processor.py:161} INFO - Started process (PID=15785) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:47:16.384+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:47:16.385+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:47:16.385+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:47:16.403+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:47:16.426+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:47:16.425+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:47:16.477+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.098 seconds
[2024-02-20T20:47:47.120+0000] {processor.py:161} INFO - Started process (PID=15842) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:47:47.121+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:47:47.122+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:47:47.122+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:47:47.140+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:47:47.158+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:47:47.158+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:47:47.199+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.082 seconds
[2024-02-20T20:48:17.823+0000] {processor.py:161} INFO - Started process (PID=15899) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:48:17.824+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:48:17.825+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:48:17.824+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:48:17.843+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:48:17.861+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:48:17.861+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:48:17.911+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.091 seconds
[2024-02-20T20:48:48.055+0000] {processor.py:161} INFO - Started process (PID=15956) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:48:48.056+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:48:48.057+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:48:48.057+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:48:48.075+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:48:48.094+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:48:48.093+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:48:48.137+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.085 seconds
[2024-02-20T20:49:18.987+0000] {processor.py:161} INFO - Started process (PID=16013) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:49:18.988+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:49:18.989+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:49:18.989+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:49:19.007+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:49:19.025+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:49:19.024+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:49:19.062+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.078 seconds
[2024-02-20T20:49:49.485+0000] {processor.py:161} INFO - Started process (PID=16070) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:49:49.485+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:49:49.486+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:49:49.486+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:49:49.504+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:49:49.523+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:49:49.523+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:49:49.572+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.091 seconds
[2024-02-20T20:50:20.030+0000] {processor.py:161} INFO - Started process (PID=16127) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:50:20.031+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:50:20.033+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:50:20.032+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:50:20.052+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:50:20.071+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:50:20.070+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:50:20.118+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.091 seconds
[2024-02-20T20:50:50.264+0000] {processor.py:161} INFO - Started process (PID=16184) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:50:50.265+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:50:50.266+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:50:50.265+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:50:50.285+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:50:50.304+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:50:50.303+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:50:50.356+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.094 seconds
[2024-02-20T20:51:21.377+0000] {processor.py:161} INFO - Started process (PID=16241) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:51:21.378+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:51:21.379+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:51:21.379+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:51:21.401+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:51:21.419+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:51:21.419+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:51:21.468+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.094 seconds
[2024-02-20T20:51:52.348+0000] {processor.py:161} INFO - Started process (PID=16298) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:51:52.349+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:51:52.350+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:51:52.349+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:51:52.368+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:51:52.386+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:51:52.385+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:51:52.433+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.090 seconds
[2024-02-20T20:52:22.855+0000] {processor.py:161} INFO - Started process (PID=16355) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:52:22.856+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:52:22.857+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:52:22.856+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:52:22.874+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:52:22.891+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:52:22.890+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:52:22.937+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.086 seconds
[2024-02-20T20:52:53.443+0000] {processor.py:161} INFO - Started process (PID=16412) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:52:53.444+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:52:53.445+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:52:53.445+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:52:53.462+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:52:53.480+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:52:53.480+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:52:53.516+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.076 seconds
[2024-02-20T20:53:23.948+0000] {processor.py:161} INFO - Started process (PID=16469) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:53:23.949+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:53:23.950+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:53:23.950+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:53:23.968+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:53:23.985+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:53:23.984+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:53:24.021+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.075 seconds
[2024-02-20T20:53:54.521+0000] {processor.py:161} INFO - Started process (PID=16526) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:53:54.522+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:53:54.523+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:53:54.523+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:53:54.541+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:53:54.556+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:53:54.556+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:53:54.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.073 seconds
[2024-02-20T20:54:25.193+0000] {processor.py:161} INFO - Started process (PID=16583) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:54:25.194+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:54:25.195+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:54:25.195+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:54:25.213+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:54:25.230+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:54:25.229+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:54:25.278+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.087 seconds
[2024-02-20T20:54:55.889+0000] {processor.py:161} INFO - Started process (PID=16646) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:54:55.890+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:54:55.890+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:54:55.890+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:54:55.908+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:54:55.926+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:54:55.926+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:54:55.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.081 seconds
[2024-02-20T20:55:26.714+0000] {processor.py:161} INFO - Started process (PID=16703) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:55:26.715+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:55:26.715+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:55:26.715+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:55:26.733+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:55:26.751+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:55:26.750+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:55:26.787+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.076 seconds
[2024-02-20T20:55:57.496+0000] {processor.py:161} INFO - Started process (PID=16760) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:55:57.497+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:55:57.497+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:55:57.497+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:55:57.515+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:55:57.602+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:55:57.602+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:55:57.612+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:55:57.612+0000] {dag.py:3055} INFO - Creating ORM DAG for processing_sales
[2024-02-20T20:55:57.622+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:55:57.621+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:55:57.647+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.154 seconds
[2024-02-20T20:56:28.363+0000] {processor.py:161} INFO - Started process (PID=16817) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:56:28.364+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:56:28.365+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:56:28.365+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:56:28.397+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:56:28.423+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:56:28.422+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:56:28.453+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:56:28.453+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:56:28.482+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.123 seconds
[2024-02-20T20:56:58.574+0000] {processor.py:161} INFO - Started process (PID=16874) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:56:58.575+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:56:58.576+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:56:58.576+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:56:58.603+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:56:58.627+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:56:58.626+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:56:58.655+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:56:58.655+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:56:58.718+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.149 seconds
[2024-02-20T20:57:29.491+0000] {processor.py:161} INFO - Started process (PID=16931) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:57:29.492+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:57:29.493+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:57:29.493+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:57:29.525+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:57:29.550+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:57:29.550+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:57:29.580+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:57:29.580+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:57:29.639+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.152 seconds
[2024-02-20T20:58:00.288+0000] {processor.py:161} INFO - Started process (PID=16988) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:58:00.289+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:58:00.291+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:58:00.291+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:58:00.325+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:58:00.353+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:58:00.353+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:58:00.389+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:58:00.389+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:58:00.456+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.174 seconds
[2024-02-20T20:58:31.394+0000] {processor.py:161} INFO - Started process (PID=17045) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:58:31.395+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:58:31.396+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:58:31.396+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:58:31.426+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:58:31.457+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:58:31.456+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T20:58:31.486+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:58:31.486+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T20:58:31.543+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.154 seconds
[2024-02-20T20:58:49.246+0000] {processor.py:161} INFO - Started process (PID=17062) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:58:49.247+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:58:49.249+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:58:49.248+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:58:49.264+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:58:49.263+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 976, in get_code
  File "<frozen importlib._bootstrap_external>", line 906, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 39
    //gcp_conn_id='google-cloud',
    ^
SyntaxError: invalid syntax
[2024-02-20T20:58:49.265+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:58:49.292+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.051 seconds
[2024-02-20T20:58:51.759+0000] {processor.py:161} INFO - Started process (PID=17072) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:58:51.760+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:58:51.762+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:58:51.761+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:58:51.799+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:58:51.795+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:58:51.800+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:58:51.824+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.070 seconds
[2024-02-20T20:59:21.932+0000] {processor.py:161} INFO - Started process (PID=17127) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:59:21.933+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:59:21.934+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:59:21.934+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:59:21.960+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:59:21.955+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:59:21.960+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:59:21.984+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.057 seconds
[2024-02-20T20:59:52.209+0000] {processor.py:161} INFO - Started process (PID=17182) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:59:52.211+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:59:52.212+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:59:52.212+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:59:52.240+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:59:52.234+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:59:52.241+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:59:52.303+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.100 seconds
[2024-02-20T20:59:58.718+0000] {processor.py:161} INFO - Started process (PID=17207) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T20:59:58.720+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T20:59:58.722+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:59:58.721+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T20:59:58.767+0000] {logging_mixin.py:188} INFO - [2024-02-20T20:59:58.761+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T20:59:58.768+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T20:59:58.794+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.081 seconds
[2024-02-20T21:00:29.007+0000] {processor.py:161} INFO - Started process (PID=17263) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:00:29.008+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:00:29.009+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:00:29.009+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:00:29.035+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:00:29.030+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T21:00:29.036+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:00:29.096+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.093 seconds
[2024-02-20T21:00:59.203+0000] {processor.py:161} INFO - Started process (PID=17319) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:00:59.204+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:00:59.206+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:00:59.205+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:00:59.234+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:00:59.229+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T21:00:59.235+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:00:59.261+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.063 seconds
[2024-02-20T21:01:29.913+0000] {processor.py:161} INFO - Started process (PID=17376) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:01:29.915+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:01:29.916+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:01:29.916+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:01:29.942+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:01:29.938+0000] {dagbag.py:348} ERROR - Failed to import: /opt/airflow/dags/process_sales.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 344, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/process_sales.py", line 24, in <module>
    load_data_to_bronze = GCSToBigQueryOperator(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/transfers/gcs_to_bigquery.py", line 234, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 437, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py", line 798, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to GCSToBigQueryOperator (task_id: load_data_to_bronze). Invalid arguments were:
**kwargs: {'google_cloud_storage_conn_id': 'google-cloud'}
[2024-02-20T21:01:29.942+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:01:30.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.094 seconds
[2024-02-20T21:01:37.319+0000] {processor.py:161} INFO - Started process (PID=17391) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:01:37.321+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:01:37.322+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:01:37.322+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:01:37.360+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:01:37.483+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:01:37.483+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:01:37.510+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:01:37.510+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T21:01:37.539+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.225 seconds
[2024-02-20T21:01:38.593+0000] {processor.py:161} INFO - Started process (PID=17393) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:01:38.595+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:01:38.596+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:01:38.595+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:01:38.632+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:01:38.647+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:01:38.646+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:01:38.678+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:01:38.677+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T21:01:38.706+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.117 seconds
[2024-02-20T21:02:09.443+0000] {processor.py:161} INFO - Started process (PID=17450) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:02:09.445+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:02:09.446+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:02:09.446+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:02:09.479+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:02:09.505+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:02:09.504+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:02:09.559+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.121 seconds
[2024-02-20T21:02:39.729+0000] {processor.py:161} INFO - Started process (PID=17507) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:02:39.731+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:02:39.732+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:02:39.732+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:02:39.762+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:02:39.786+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:02:39.785+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:02:39.846+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.121 seconds
[2024-02-20T21:03:09.964+0000] {processor.py:161} INFO - Started process (PID=17564) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:03:09.965+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:03:09.967+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:03:09.966+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:03:09.999+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:03:10.025+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:03:10.025+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:03:10.071+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.111 seconds
[2024-02-20T21:03:40.354+0000] {processor.py:161} INFO - Started process (PID=17620) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:03:40.355+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:03:40.358+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:03:40.357+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:03:40.389+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:03:40.416+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:03:40.416+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:03:40.512+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.164 seconds
[2024-02-20T21:04:10.600+0000] {processor.py:161} INFO - Started process (PID=17677) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:04:10.601+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:04:10.603+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:04:10.602+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:04:10.628+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:04:10.653+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:04:10.653+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:04:10.738+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.142 seconds
[2024-02-20T21:04:40.815+0000] {processor.py:161} INFO - Started process (PID=17734) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:04:40.816+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:04:40.818+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:04:40.817+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:04:40.847+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:04:40.870+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:04:40.870+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:04:40.921+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.110 seconds
[2024-02-20T21:05:11.319+0000] {processor.py:161} INFO - Started process (PID=17791) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:05:11.320+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:05:11.321+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:05:11.321+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:05:11.349+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:05:11.372+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:05:11.372+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:05:11.468+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.153 seconds
[2024-02-20T21:05:41.757+0000] {processor.py:161} INFO - Started process (PID=17848) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:05:41.758+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:05:41.760+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:05:41.759+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:05:41.789+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:05:41.815+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:05:41.815+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:05:41.871+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.120 seconds
[2024-02-20T21:06:11.913+0000] {processor.py:161} INFO - Started process (PID=17905) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:06:11.915+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:06:11.916+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:06:11.916+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:06:11.948+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:06:11.973+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:06:11.972+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:06:12.037+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.130 seconds
[2024-02-20T21:06:42.451+0000] {processor.py:161} INFO - Started process (PID=17962) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:06:42.452+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:06:42.454+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:06:42.453+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:06:42.480+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:06:42.503+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:06:42.502+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:06:42.587+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.140 seconds
[2024-02-20T21:07:12.979+0000] {processor.py:161} INFO - Started process (PID=18019) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:07:12.980+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:07:12.982+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:07:12.981+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:07:13.012+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:07:13.036+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:07:13.036+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:07:13.096+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.121 seconds
[2024-02-20T21:07:43.822+0000] {processor.py:161} INFO - Started process (PID=18076) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:07:43.823+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:07:43.825+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:07:43.824+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:07:43.860+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:07:43.907+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:07:43.906+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:07:44.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.195 seconds
[2024-02-20T21:08:14.536+0000] {processor.py:161} INFO - Started process (PID=18133) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:08:14.537+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:08:14.538+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:08:14.538+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:08:14.574+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:08:14.602+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:08:14.602+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:08:14.699+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.169 seconds
[2024-02-20T21:08:45.576+0000] {processor.py:161} INFO - Started process (PID=18190) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:08:45.577+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:08:45.578+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:08:45.578+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:08:45.609+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:08:45.637+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:08:45.637+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:08:45.694+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.123 seconds
[2024-02-20T21:09:16.023+0000] {processor.py:161} INFO - Started process (PID=18247) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:09:16.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:09:16.026+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:09:16.026+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:09:16.056+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:09:16.080+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:09:16.080+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:09:16.127+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.108 seconds
[2024-02-20T21:09:46.573+0000] {processor.py:161} INFO - Started process (PID=18304) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:09:46.574+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:09:46.575+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:09:46.575+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:09:46.602+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:09:46.626+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:09:46.626+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:09:46.712+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.143 seconds
[2024-02-20T21:10:16.839+0000] {processor.py:161} INFO - Started process (PID=18361) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:10:16.840+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:10:16.842+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:10:16.841+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:10:16.874+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:10:16.899+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:10:16.898+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:10:16.952+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.117 seconds
[2024-02-20T21:10:47.841+0000] {processor.py:161} INFO - Started process (PID=18418) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:10:47.842+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:10:47.843+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:10:47.843+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:10:47.873+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:10:47.995+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:10:47.994+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:10:48.008+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:10:48.008+0000] {dag.py:3055} INFO - Creating ORM DAG for processing_sales
[2024-02-20T21:10:48.023+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:10:48.022+0000] {dag.py:3820} INFO - Setting next_dagrun for processing_sales to 2022-08-01T01:00:00+00:00, run_after=2022-08-02T01:00:00+00:00
[2024-02-20T21:10:48.045+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.209 seconds
[2024-02-20T21:11:18.336+0000] {processor.py:161} INFO - Started process (PID=18475) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:11:18.338+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:11:18.339+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:11:18.339+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:11:18.379+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:11:18.407+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:11:18.407+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:11:18.469+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.136 seconds
[2024-02-20T21:11:48.936+0000] {processor.py:161} INFO - Started process (PID=18532) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:11:48.938+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:11:48.940+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:11:48.939+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:11:48.971+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:11:48.998+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:11:48.998+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:11:49.074+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.143 seconds
[2024-02-20T21:12:19.702+0000] {processor.py:161} INFO - Started process (PID=18589) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:12:19.703+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:12:19.704+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:12:19.704+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:12:19.731+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:12:19.754+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:12:19.753+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:12:19.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.142 seconds
[2024-02-20T21:12:50.597+0000] {processor.py:161} INFO - Started process (PID=18646) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:12:50.598+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:12:50.599+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:12:50.599+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:12:50.627+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:12:50.653+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:12:50.653+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:12:50.702+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.110 seconds
[2024-02-20T21:13:21.354+0000] {processor.py:161} INFO - Started process (PID=18703) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:13:21.355+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:13:21.356+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:13:21.356+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:13:21.388+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:13:21.412+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:13:21.412+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:13:21.498+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.148 seconds
[2024-02-20T21:13:51.650+0000] {processor.py:161} INFO - Started process (PID=18760) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:13:51.651+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:13:51.653+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:13:51.652+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:13:51.684+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:13:51.713+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:13:51.713+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:13:51.771+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.127 seconds
[2024-02-20T21:14:21.875+0000] {processor.py:161} INFO - Started process (PID=18817) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:14:21.876+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:14:21.878+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:14:21.877+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:14:21.911+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:14:21.935+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:14:21.935+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:14:21.990+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.120 seconds
[2024-02-20T21:14:52.203+0000] {processor.py:161} INFO - Started process (PID=18874) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:14:52.204+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:14:52.206+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:14:52.206+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:14:52.244+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:14:52.272+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:14:52.272+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:14:52.322+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.127 seconds
[2024-02-20T21:15:22.471+0000] {processor.py:161} INFO - Started process (PID=18931) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:15:22.472+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:15:22.474+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:15:22.473+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:15:22.501+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:15:22.529+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:15:22.529+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:15:22.621+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.155 seconds
[2024-02-20T21:15:53.304+0000] {processor.py:161} INFO - Started process (PID=18988) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:15:53.306+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:15:53.307+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:15:53.306+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:15:53.332+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:15:53.354+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:15:53.354+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:15:53.400+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.100 seconds
[2024-02-20T21:16:23.966+0000] {processor.py:161} INFO - Started process (PID=19045) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:16:23.967+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:16:23.968+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:16:23.968+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:16:23.995+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:16:24.022+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:16:24.021+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:16:24.103+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.142 seconds
[2024-02-20T21:16:54.145+0000] {processor.py:161} INFO - Started process (PID=19102) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:16:54.146+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:16:54.147+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:16:54.147+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:16:54.174+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:16:54.205+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:16:54.205+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:16:54.254+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.113 seconds
[2024-02-20T21:17:24.338+0000] {processor.py:161} INFO - Started process (PID=19159) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:17:24.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:17:24.340+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:17:24.340+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:17:24.370+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:17:24.394+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:17:24.393+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:17:24.485+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.151 seconds
[2024-02-20T21:17:54.965+0000] {processor.py:161} INFO - Started process (PID=19216) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:17:54.967+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:17:54.968+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:17:54.968+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:17:54.999+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:17:55.040+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:17:55.039+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:17:55.106+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.146 seconds
[2024-02-20T21:18:25.220+0000] {processor.py:161} INFO - Started process (PID=19273) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:18:25.222+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:18:25.223+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:18:25.222+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:18:25.250+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:18:25.278+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:18:25.277+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:18:25.359+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.143 seconds
[2024-02-20T21:19:41.227+0000] {processor.py:161} INFO - Started process (PID=35) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:19:41.235+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:19:41.253+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:19:41.252+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:19:41.412+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:19:41.802+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:19:41.801+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:19:42.438+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 1.235 seconds
[2024-02-20T21:20:05.330+0000] {processor.py:161} INFO - Started process (PID=35) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:20:05.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:20:05.336+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:20:05.336+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:20:05.382+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:20:05.598+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:20:05.597+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:20:05.653+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.330 seconds
[2024-02-20T21:20:35.804+0000] {processor.py:161} INFO - Started process (PID=92) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:20:35.805+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:20:35.809+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:20:35.808+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:20:35.841+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:20:35.871+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:20:35.869+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:20:35.952+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.154 seconds
[2024-02-20T21:21:06.002+0000] {processor.py:161} INFO - Started process (PID=149) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:21:06.003+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:21:06.006+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:21:06.006+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:21:06.033+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:21:06.062+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:21:06.061+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:21:06.119+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.121 seconds
[2024-02-20T21:21:37.041+0000] {processor.py:161} INFO - Started process (PID=206) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:21:37.043+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:21:37.049+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:21:37.048+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:21:37.092+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:21:37.135+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:21:37.133+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:21:37.215+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.180 seconds
[2024-02-20T21:22:07.406+0000] {processor.py:161} INFO - Started process (PID=263) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:22:07.407+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:22:07.412+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:22:07.411+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:22:07.442+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:22:07.471+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:22:07.471+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:22:07.529+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.128 seconds
[2024-02-20T21:22:37.902+0000] {processor.py:161} INFO - Started process (PID=320) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:22:37.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:22:37.909+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:22:37.908+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:22:37.937+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:22:37.965+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:22:37.964+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:22:38.032+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.136 seconds
[2024-02-20T21:23:08.581+0000] {processor.py:161} INFO - Started process (PID=377) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:23:08.582+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:23:08.587+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:23:08.587+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:23:08.622+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:23:08.651+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:23:08.650+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:23:08.732+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.156 seconds
[2024-02-20T21:23:39.196+0000] {processor.py:161} INFO - Started process (PID=434) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:23:39.198+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:23:39.205+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:23:39.204+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:23:39.235+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:23:39.263+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:23:39.263+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:23:39.358+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.170 seconds
[2024-02-20T21:24:10.078+0000] {processor.py:161} INFO - Started process (PID=491) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:24:10.080+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:24:10.084+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:24:10.084+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:24:10.113+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:24:10.148+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:24:10.147+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:24:10.264+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.190 seconds
[2024-02-20T21:24:41.219+0000] {processor.py:161} INFO - Started process (PID=548) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:24:41.221+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:24:41.225+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:24:41.224+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:24:41.256+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:24:41.282+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:24:41.281+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:24:41.367+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.152 seconds
[2024-02-20T21:25:12.268+0000] {processor.py:161} INFO - Started process (PID=605) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:25:12.270+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:25:12.276+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:25:12.276+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:25:12.317+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:25:12.348+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:25:12.347+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:25:12.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.197 seconds
[2024-02-20T21:25:42.557+0000] {processor.py:161} INFO - Started process (PID=662) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:25:42.558+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:25:42.563+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:25:42.562+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:25:42.591+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:25:42.617+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:25:42.616+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:25:42.701+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.148 seconds
[2024-02-20T21:26:13.521+0000] {processor.py:161} INFO - Started process (PID=719) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:26:13.523+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:26:13.527+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:26:13.527+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:26:13.556+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:26:13.584+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:26:13.583+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:26:13.645+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.130 seconds
[2024-02-20T21:26:44.000+0000] {processor.py:161} INFO - Started process (PID=776) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:26:44.001+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:26:44.006+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:26:44.005+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:26:44.038+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:26:44.063+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:26:44.063+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:26:44.154+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.159 seconds
[2024-02-20T21:27:14.218+0000] {processor.py:161} INFO - Started process (PID=833) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:27:14.219+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:27:14.224+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:27:14.224+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:27:14.253+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:27:14.282+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:27:14.281+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:27:14.369+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.156 seconds
[2024-02-20T21:27:44.446+0000] {processor.py:161} INFO - Started process (PID=890) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:27:44.447+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:27:44.451+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:27:44.451+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:27:44.481+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:27:44.506+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:27:44.505+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:27:44.597+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.155 seconds
[2024-02-20T21:28:15.084+0000] {processor.py:161} INFO - Started process (PID=947) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:28:15.087+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:28:15.092+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:28:15.092+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:28:15.126+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:28:15.158+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:28:15.158+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:28:15.259+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.180 seconds
[2024-02-20T21:28:45.592+0000] {processor.py:161} INFO - Started process (PID=1004) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:28:45.594+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:28:45.598+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:28:45.598+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:28:45.633+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:28:45.661+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:28:45.660+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:28:45.751+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.165 seconds
[2024-02-20T21:29:15.923+0000] {processor.py:161} INFO - Started process (PID=1061) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:29:15.924+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:29:15.929+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:29:15.928+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:29:15.964+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:29:15.999+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:29:15.998+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:29:16.124+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.208 seconds
[2024-02-20T21:29:46.718+0000] {processor.py:161} INFO - Started process (PID=1118) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:29:46.720+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:29:46.725+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:29:46.725+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:29:46.757+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:29:46.784+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:29:46.784+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:29:46.875+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.162 seconds
[2024-02-20T21:30:17.464+0000] {processor.py:161} INFO - Started process (PID=1175) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:30:17.466+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:30:17.470+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:30:17.469+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:30:17.502+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:30:17.532+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:30:17.531+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:30:17.611+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.152 seconds
[2024-02-20T21:30:47.735+0000] {processor.py:161} INFO - Started process (PID=1232) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:30:47.737+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:30:47.743+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:30:47.742+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:30:47.773+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:30:47.799+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:30:47.798+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:30:47.889+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.160 seconds
[2024-02-20T21:31:18.372+0000] {processor.py:161} INFO - Started process (PID=1289) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:31:18.374+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:31:18.378+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:31:18.378+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:31:18.409+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:31:18.440+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:31:18.439+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:31:18.506+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.139 seconds
[2024-02-20T21:31:48.876+0000] {processor.py:161} INFO - Started process (PID=1346) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:31:48.877+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:31:48.883+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:31:48.882+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:31:48.914+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:31:48.941+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:31:48.941+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:31:49.197+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.328 seconds
[2024-02-20T21:32:19.541+0000] {processor.py:161} INFO - Started process (PID=1403) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:32:19.543+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:32:19.547+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:32:19.546+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:32:19.578+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:32:19.604+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:32:19.603+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:32:19.846+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.312 seconds
[2024-02-20T21:32:50.373+0000] {processor.py:161} INFO - Started process (PID=1460) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:32:50.375+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:32:50.379+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:32:50.378+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:32:50.407+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:32:50.432+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:32:50.432+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:32:50.537+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.169 seconds
[2024-02-20T21:33:21.079+0000] {processor.py:161} INFO - Started process (PID=1523) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:33:21.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:33:21.084+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:33:21.083+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:33:21.119+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:33:21.152+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:33:21.151+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:33:21.257+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.185 seconds
[2024-02-20T21:33:52.025+0000] {processor.py:161} INFO - Started process (PID=1580) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:33:52.027+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:33:52.031+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:33:52.031+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:33:52.058+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:33:52.277+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:33:52.277+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:33:52.366+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.345 seconds
[2024-02-20T21:34:22.763+0000] {processor.py:161} INFO - Started process (PID=1637) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:34:22.764+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:34:22.769+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:34:22.768+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:34:22.797+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:34:22.828+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:34:22.827+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:34:23.064+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.307 seconds
[2024-02-20T21:34:53.529+0000] {processor.py:161} INFO - Started process (PID=1694) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:34:53.530+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:34:53.534+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:34:53.534+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:34:53.559+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:34:53.584+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:34:53.584+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:34:53.798+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.275 seconds
[2024-02-20T21:35:24.288+0000] {processor.py:161} INFO - Started process (PID=1751) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:35:24.289+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:35:24.293+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:35:24.293+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:35:24.321+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:35:24.349+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:35:24.348+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:35:24.402+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.118 seconds
[2024-02-20T21:35:54.725+0000] {processor.py:161} INFO - Started process (PID=1808) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:35:54.726+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:35:54.730+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:35:54.729+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:35:54.758+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:35:54.785+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:35:54.784+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:35:54.862+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.141 seconds
[2024-02-20T21:36:25.499+0000] {processor.py:161} INFO - Started process (PID=1865) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:36:25.502+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:36:25.507+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:36:25.506+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:36:25.549+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:36:25.587+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:36:25.586+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:36:25.657+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.164 seconds
[2024-02-20T21:36:56.315+0000] {processor.py:161} INFO - Started process (PID=1922) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:36:56.317+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:36:56.321+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:36:56.320+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:36:56.351+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:36:56.378+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:36:56.377+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:36:56.429+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.119 seconds
[2024-02-20T21:37:26.737+0000] {processor.py:161} INFO - Started process (PID=1979) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:37:26.738+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:37:26.743+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:37:26.742+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:37:26.770+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:37:26.800+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:37:26.799+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:37:26.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.128 seconds
[2024-02-20T21:37:56.924+0000] {processor.py:161} INFO - Started process (PID=2036) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:37:56.926+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:37:56.931+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:37:56.930+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:37:56.969+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:37:57.004+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:37:57.003+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:37:57.087+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.167 seconds
[2024-02-20T21:38:27.213+0000] {processor.py:161} INFO - Started process (PID=2093) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:38:27.214+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:38:27.218+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:38:27.217+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:38:27.244+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:38:27.270+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:38:27.270+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:38:27.346+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.138 seconds
[2024-02-20T21:38:57.764+0000] {processor.py:161} INFO - Started process (PID=2151) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:38:57.765+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:38:57.771+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:38:57.771+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:38:57.807+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:38:57.841+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:38:57.840+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:38:57.916+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.158 seconds
[2024-02-20T21:39:28.495+0000] {processor.py:161} INFO - Started process (PID=2208) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:39:28.497+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:39:28.504+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:39:28.503+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:39:28.557+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:39:28.597+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:39:28.596+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:39:28.666+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.178 seconds
[2024-02-20T21:39:59.099+0000] {processor.py:161} INFO - Started process (PID=2265) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:39:59.101+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:39:59.106+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:39:59.106+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:39:59.143+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:39:59.172+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:39:59.171+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:39:59.229+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.135 seconds
[2024-02-20T21:40:29.825+0000] {processor.py:161} INFO - Started process (PID=2322) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:40:29.827+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:40:29.835+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:40:29.834+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:40:29.887+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:40:29.931+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:40:29.930+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:40:30.039+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.221 seconds
[2024-02-20T21:41:00.331+0000] {processor.py:161} INFO - Started process (PID=2379) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:41:00.334+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:41:00.339+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:41:00.339+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:41:00.384+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:41:00.422+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:41:00.421+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:41:00.503+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.178 seconds
[2024-02-20T21:41:24.597+0000] {processor.py:161} INFO - Started process (PID=2436) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:41:24.599+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:41:24.603+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:41:24.603+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:41:24.657+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:41:24.688+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:41:24.687+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:41:24.752+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.161 seconds
[2024-02-20T21:41:54.933+0000] {processor.py:161} INFO - Started process (PID=2493) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:41:54.935+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:41:54.938+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:41:54.937+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:41:54.973+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:41:55.008+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:41:55.007+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:41:55.081+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.155 seconds
[2024-02-20T21:42:25.422+0000] {processor.py:161} INFO - Started process (PID=2550) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:42:25.423+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:42:25.425+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:42:25.425+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:42:25.452+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:42:25.476+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:42:25.476+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:42:25.562+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.145 seconds
[2024-02-20T21:42:56.157+0000] {processor.py:161} INFO - Started process (PID=2607) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:42:56.158+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:42:56.161+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:42:56.161+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:42:56.186+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:42:56.214+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:42:56.213+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:42:56.301+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.149 seconds
[2024-02-20T21:43:26.372+0000] {processor.py:161} INFO - Started process (PID=2665) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:43:26.373+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:43:26.378+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:43:26.377+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:43:26.403+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:43:26.426+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:43:26.426+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:43:26.506+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.138 seconds
[2024-02-20T21:43:57.107+0000] {processor.py:161} INFO - Started process (PID=2722) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:43:57.108+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:43:57.111+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:43:57.111+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:43:57.141+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:43:57.166+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:43:57.166+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:43:57.253+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.150 seconds
[2024-02-20T21:44:27.489+0000] {processor.py:161} INFO - Started process (PID=2779) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:44:27.490+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:44:27.493+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:44:27.492+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:44:27.522+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:44:27.554+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:44:27.553+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:44:27.647+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.163 seconds
[2024-02-20T21:44:38.154+0000] {processor.py:161} INFO - Started process (PID=2798) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:44:38.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:44:38.158+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:44:38.158+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:44:38.195+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:44:38.226+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:44:38.226+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:44:38.285+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.135 seconds
[2024-02-20T21:45:08.743+0000] {processor.py:161} INFO - Started process (PID=2852) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:45:08.745+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:45:08.749+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:45:08.748+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:45:08.775+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:45:08.801+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:45:08.800+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:45:08.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.149 seconds
[2024-02-20T21:45:38.971+0000] {processor.py:161} INFO - Started process (PID=2909) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:45:38.973+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:45:38.977+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:45:38.976+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:45:39.002+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:45:39.029+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:45:39.028+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:45:39.120+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.154 seconds
[2024-02-20T21:46:09.425+0000] {processor.py:161} INFO - Started process (PID=2966) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:46:09.427+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:46:09.431+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:46:09.430+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:46:09.464+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:46:09.494+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:46:09.493+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:46:09.604+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.184 seconds
[2024-02-20T21:46:40.385+0000] {processor.py:161} INFO - Started process (PID=3023) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:46:40.387+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:46:40.391+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:46:40.391+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:46:40.423+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:46:40.455+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:46:40.454+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:46:40.527+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.147 seconds
[2024-02-20T21:47:10.720+0000] {processor.py:161} INFO - Started process (PID=3080) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:47:10.722+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:47:10.725+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:47:10.725+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:47:10.750+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:47:10.778+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:47:10.777+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:47:10.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.150 seconds
[2024-02-20T21:47:41.213+0000] {processor.py:161} INFO - Started process (PID=3137) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:47:41.215+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:47:41.219+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:47:41.219+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:47:41.245+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:47:41.272+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:47:41.271+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:47:41.365+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.157 seconds
[2024-02-20T21:48:04.244+0000] {processor.py:161} INFO - Started process (PID=3179) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:48:04.246+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:48:04.253+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:48:04.252+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:48:04.302+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:48:04.495+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:48:04.494+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:48:04.586+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.350 seconds
[2024-02-20T21:48:34.978+0000] {processor.py:161} INFO - Started process (PID=3236) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:48:34.980+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:48:34.984+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:48:34.983+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:48:35.017+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:48:35.048+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:48:35.047+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:48:35.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.137 seconds
[2024-02-20T21:49:05.359+0000] {processor.py:161} INFO - Started process (PID=3293) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:49:05.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:49:05.363+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:49:05.363+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:49:05.394+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:49:05.419+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:49:05.419+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:49:05.519+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.166 seconds
[2024-02-20T21:49:35.960+0000] {processor.py:161} INFO - Started process (PID=3350) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:49:35.961+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:49:35.964+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:49:35.963+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:49:35.988+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:49:36.018+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:49:36.018+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:49:36.071+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.116 seconds
[2024-02-20T21:50:06.613+0000] {processor.py:161} INFO - Started process (PID=3407) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:50:06.614+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:50:06.618+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:50:06.617+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:50:06.650+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:50:06.684+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:50:06.683+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:50:06.787+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.181 seconds
[2024-02-20T21:50:36.904+0000] {processor.py:161} INFO - Started process (PID=3464) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:50:36.905+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:50:36.908+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:50:36.908+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:50:36.936+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:50:36.961+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:50:36.961+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:50:37.007+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.107 seconds
[2024-02-20T21:51:07.429+0000] {processor.py:161} INFO - Started process (PID=3521) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:51:07.430+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:51:07.434+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:51:07.433+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:51:07.461+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:51:07.489+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:51:07.488+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:51:07.539+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.115 seconds
[2024-02-20T21:51:37.816+0000] {processor.py:161} INFO - Started process (PID=3578) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:51:37.818+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:51:37.823+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:51:37.822+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:51:37.862+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:51:37.894+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:51:37.893+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:51:37.979+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.168 seconds
[2024-02-20T21:52:08.068+0000] {processor.py:161} INFO - Started process (PID=3635) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:52:08.069+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:52:08.073+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:52:08.072+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:52:08.101+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:52:08.128+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:52:08.127+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:52:08.220+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.157 seconds
[2024-02-20T21:52:38.405+0000] {processor.py:161} INFO - Started process (PID=3692) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:52:38.407+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:52:38.410+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:52:38.409+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:52:38.438+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:52:38.465+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:52:38.464+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:52:38.516+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.116 seconds
[2024-02-20T21:53:08.928+0000] {processor.py:161} INFO - Started process (PID=3749) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:53:08.929+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:53:08.933+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:53:08.932+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:53:08.973+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:53:09.006+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:53:09.005+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:53:09.073+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.151 seconds
[2024-02-20T21:53:39.772+0000] {processor.py:161} INFO - Started process (PID=3806) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:53:39.773+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:53:39.777+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:53:39.776+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:53:39.806+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:53:39.832+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:53:39.831+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:53:39.932+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.165 seconds
[2024-02-20T21:54:10.525+0000] {processor.py:161} INFO - Started process (PID=3863) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:54:10.527+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:54:10.530+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:54:10.529+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:54:10.558+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:54:10.588+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:54:10.587+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:54:10.672+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.151 seconds
[2024-02-20T21:54:40.721+0000] {processor.py:161} INFO - Started process (PID=3920) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:54:40.722+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:54:40.725+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:54:40.725+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:54:40.755+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:54:40.782+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:54:40.782+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:54:40.849+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.132 seconds
[2024-02-20T21:55:11.076+0000] {processor.py:161} INFO - Started process (PID=3976) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:55:11.078+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:55:11.081+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:55:11.081+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:55:11.109+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:55:11.135+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:55:11.135+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:55:11.222+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.149 seconds
[2024-02-20T21:55:41.367+0000] {processor.py:161} INFO - Started process (PID=4033) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:55:41.368+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:55:41.371+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:55:41.370+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:55:41.398+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:55:41.423+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:55:41.422+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:55:41.504+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.141 seconds
[2024-02-20T21:56:11.727+0000] {processor.py:161} INFO - Started process (PID=4090) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:56:11.729+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:56:11.731+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:56:11.731+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:56:11.762+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:56:11.789+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:56:11.789+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:56:11.868+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.144 seconds
[2024-02-20T21:56:42.781+0000] {processor.py:161} INFO - Started process (PID=4147) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:56:42.783+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:56:42.787+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:56:42.786+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:56:42.821+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:56:42.849+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:56:42.849+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:56:42.904+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.128 seconds
[2024-02-20T21:57:13.875+0000] {processor.py:161} INFO - Started process (PID=4204) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T21:57:13.877+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T21:57:13.880+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:57:13.880+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T21:57:13.911+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T21:57:13.936+0000] {logging_mixin.py:188} INFO - [2024-02-20T21:57:13.936+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T21:57:13.986+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.117 seconds
[2024-02-20T23:34:51.008+0000] {processor.py:161} INFO - Started process (PID=4266) to work on /opt/airflow/dags/process_sales.py
[2024-02-20T23:34:51.015+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/process_sales.py for tasks to queue
[2024-02-20T23:34:51.029+0000] {logging_mixin.py:188} INFO - [2024-02-20T23:34:51.029+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/process_sales.py
[2024-02-20T23:34:51.303+0000] {processor.py:840} INFO - DAG(s) 'processing_sales' retrieved from /opt/airflow/dags/process_sales.py
[2024-02-20T23:34:51.406+0000] {logging_mixin.py:188} INFO - [2024-02-20T23:34:51.404+0000] {dag.py:3033} INFO - Sync 1 DAGs
[2024-02-20T23:34:51.689+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/process_sales.py took 0.687 seconds
